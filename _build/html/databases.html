
<!DOCTYPE html>


<html lang="en" data-content_root="./" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>Privacy in databases &#8212; Privacy Book</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="_static/styles/theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="_static/styles/bootstrap.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="_static/styles/pydata-sphinx-theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />

  
  <link href="_static/vendor/fontawesome/6.5.2/css/all.min.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.5.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.5.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.5.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="_static/pygments.css?v=03e43079" />
    <link rel="stylesheet" type="text/css" href="_static/styles/sphinx-book-theme.css?v=eba8b062" />
    <link rel="stylesheet" type="text/css" href="_static/togglebutton.css?v=13237357" />
    <link rel="stylesheet" type="text/css" href="_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="_static/mystnb.8ecb98da25f57f5357bf6f572d296f466b2cfe2517ffebfabe82451661e28f02.css?v=6644e6bb" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-thebe.css?v=4fa983c6" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-design.min.css?v=95c83b7e" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b" />
<link rel="preload" as="script" href="_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b" />
  <script src="_static/vendor/fontawesome/6.5.2/js/all.min.js?digest=dfe6caa3a7d634c4db9b"></script>

    <script src="_static/documentation_options.js?v=9eb32ce0"></script>
    <script src="_static/doctools.js?v=9a2dae69"></script>
    <script src="_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="_static/copybutton.js?v=f281be69"></script>
    <script src="_static/scripts/sphinx-book-theme.js?v=887ef09a"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="_static/togglebutton.js?v=4a39c7ea"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="_static/design-tabs.js?v=f930bc37"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script async="async" src="_static/sphinx-thebe.js?v=c100c467"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'databases';</script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="Anonymization library" href="anonymization-library.html" />
    <link rel="prev" title="Privacy enhancing techniques" href="pets.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-primary-sidebar-checkbox"/>
  <label class="overlay overlay-primary" for="pst-primary-sidebar-checkbox"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-secondary-sidebar-checkbox"/>
  <label class="overlay overlay-secondary" for="pst-secondary-sidebar-checkbox"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>

  
    <header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  
    
  

<a class="navbar-brand logo" href="intro.html">
  
  
  
  
  
    
    
      
    
    
    <img src="_static/logo_v01-original.png" class="logo__image only-light" alt="Privacy Book - Home"/>
    <script>document.write(`<img src="_static/logo_v01-original.png" class="logo__image only-dark" alt="Privacy Book - Home"/>`);</script>
  
  
</a></div>
        <div class="sidebar-primary-item">

 <script>
 document.write(`
   <button class="btn search-button-field search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
    <span class="search-button__default-text">Search</span>
    <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
   </button>
 `);
 </script></div>
        <div class="sidebar-primary-item"><nav class="bd-links bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="intro.html">
                    Privacy engineering course
                </a>
            </li>
        </ul>
        <ul class="current nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="intro-gdpr.html">Introduction and GDPR</a></li>
<li class="toctree-l1"><a class="reference internal" href="privacy-by-design.html">Privacy by design</a></li>
<li class="toctree-l1"><a class="reference internal" href="pets.html">Privacy enhancing techniques</a></li>
<li class="toctree-l1 current active has-children"><a class="current reference internal" href="#">Privacy in databases</a><details open="open"><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="anonymization-library.html">Anonymization library</a></li>
<li class="toctree-l2"><a class="reference internal" href="notebooks/test_anonymization.html">Anonymization examples</a></li>
</ul>
</details></li>
<li class="toctree-l1"><a class="reference internal" href="unstructured.html">Privacy in unstructured data</a></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="privacy-ml.html">Privacy in machine learning</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="notebooks/LipariSC_MIA.html">Salem paper attacks</a></li>
<li class="toctree-l2"><a class="reference internal" href="notebooks/FederatedLearning-sklearn.html">Federated Learning and Label Flipping attacks</a></li>





<li class="toctree-l2"><a class="reference internal" href="notebooks/unlearning-CIFAR10.html">NeurIPS 2023 Machine Unlearning Challenge Starting Kit</a></li>



</ul>
</details></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><button class="sidebar-toggle primary-toggle btn btn-sm" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</button></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-source-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Source repositories">
    <i class="fab fa-github"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://github.com/ablancoj/privacy-course" target="_blank"
   class="btn btn-sm btn-source-repository-button dropdown-item"
   title="Source repository"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="btn__text-container">Repository</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/ablancoj/privacy-course/issues/new?title=Issue%20on%20page%20%2Fdatabases.html&body=Your%20issue%20content%20here." target="_blank"
   class="btn btn-sm btn-source-issues-button dropdown-item"
   title="Open an issue"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="btn__text-container">Open issue</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="_sources/databases.md" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.md</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<script>
document.write(`
  <button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="theme-switch fa-solid fa-sun fa-lg" data-mode="light"></i>
    <i class="theme-switch fa-solid fa-moon fa-lg" data-mode="dark"></i>
    <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"></i>
  </button>
`);
</script>


<script>
document.write(`
  <button class="btn btn-sm pst-navbar-icon search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
  </button>
`);
</script>
<button class="sidebar-toggle secondary-toggle btn btn-sm" title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</button>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Privacy in databases</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id1">Privacy in Databases</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#respondent-privacy-statistical-disclosure-control">Respondent Privacy: Statistical Disclosure Control</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#non-perturbative-masking">Non-perturbative Masking</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#perturbative-masking">Perturbative Masking</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#synthetic-microdata-generation">Synthetic Microdata Generation</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#privacy-models">Privacy Models</a><ul class="nav section-nav flex-column">
<li class="toc-h5 nav-item toc-entry"><a class="reference internal nav-link" href="#k-anonymity-and-extensions">k-Anonymity and Extensions</a></li>
<li class="toc-h5 nav-item toc-entry"><a class="reference internal nav-link" href="#differential-privacy">Differential Privacy</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#introduction">Introduction</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#tabular-data-protection">Tabular data protection</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#types-of-tables">Types of tables</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#disclosure-attacks-in-tables">Disclosure attacks in tables</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#sdc-methods-for-tables">SDC Methods for tables</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#cell-suppression">Cell suppression</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#sensitivity-rules">Sensitivity rules</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#secondary-suppressions">Secondary suppressions</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#controlled-rounding-and-controlled-tabular-adjustment">Controlled rounding and Controlled tabular adjustment</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#utility-loss-in-tabular-sdc">Utility loss in tabular SDC</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#disclosure-risk-in-tabular-sdc">Disclosure risk in tabular SDC</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#interactive-databases">Interactive databases</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#issues">Issues</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#interactive-database-protection">Interactive database protection</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#query-restriction">Query restriction</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#camouflage">Camouflage</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#query-perturbation">Query perturbation</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#randomized-response">Randomized response</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#id2">Differential privacy</a><ul class="nav section-nav flex-column">
<li class="toc-h5 nav-item toc-entry"><a class="reference internal nav-link" href="#the-laplace-mechanism">The Laplace mechanism</a></li>
<li class="toc-h5 nav-item toc-entry"><a class="reference internal nav-link" href="#properties-of-dp">Properties of DP</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#utility-loss-in-interactive-databases">Utility loss in interactive databases</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#disclosure-risk-in-interactive-databases">Disclosure risk in interactive databases</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#microdata">Microdata</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id3">Non-perturbative masking</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#recoding">Recoding</a><ul class="nav section-nav flex-column">
<li class="toc-h5 nav-item toc-entry"><a class="reference internal nav-link" href="#global-recoding">Global recoding</a></li>
<li class="toc-h5 nav-item toc-entry"><a class="reference internal nav-link" href="#top-bottom-coding">Top/bottom coding</a></li>
<li class="toc-h5 nav-item toc-entry"><a class="reference internal nav-link" href="#rounding">Rounding</a></li>
</ul>
</li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#local-suppression">Local suppression</a><ul class="nav section-nav flex-column">
<li class="toc-h5 nav-item toc-entry"><a class="reference internal nav-link" href="#recoding-suppression-and-k-anonymity">Recoding, suppression, and ùëò-anonymity</a></li>
</ul>
</li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#pram">PRAM</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#microaggregation">Microaggregation</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#noise-addition">Noise addition</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#swapping">Swapping</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#synthetic-data-generation">Synthetic data generation</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#utility-loss-in-microdata-protection">Utility loss in microdata protection</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#disclosure-risk-in-microdata-protection">Disclosure risk in microdata protection</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#a-priori-disclosure-risk">A priori disclosure risk</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#a-posteriori-disclosure-risk">A posteriori disclosure risk</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#owner-privacy-privacy-preserving-data-mining">Owner Privacy: Privacy-Preserving Data Mining</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#user-privacy-private-information-retrieval">User Privacy: Private Information Retrieval</a></li>
</ul>
</li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <section class="tex2jax_ignore mathjax_ignore" id="privacy-in-databases">
<h1>Privacy in databases<a class="headerlink" href="#privacy-in-databases" title="Link to this heading">#</a></h1>
<p>Anonymization of structured databases is one of the central technical methods for protecting personal data while preserving its analytical or statistical value. Structured data, such as that stored in relational databases or spreadsheets, consists of well-defined attributes arranged in rows and columns‚Äîeach row representing an individual and each column representing an attribute (such as age, income, or medical condition). These datasets are the foundation of much statistical, social, and biomedical research, yet they also pose serious privacy risks because individuals can often be identified or their attributes inferred, even after direct identifiers have been removed. The purpose of anonymization is to transform such data so that it can be used, published, or shared with third parties without revealing or allowing the inference of personal information.</p>
<p>To understand how anonymization works, it is essential to distinguish between different types of data attributes. Direct identifiers are attributes that uniquely identify a person on their own‚Äîsuch as a name, personal identification number, email address, or phone number. These are typically removed entirely in any anonymization process. Quasi-identifiers (or indirect identifiers) are attributes that do not identify an individual by themselves but can do so when combined with other information. Common examples include combinations of birth date, postal code, and gender, which have been shown to uniquely identify large portions of a population when cross-referenced with external data sources. Confidential attributes are those that are sensitive in nature, such as salary, medical diagnosis, or political opinion; these are not used to identify individuals but must be protected against disclosure. Finally, non-confidential attributes are those that carry little privacy risk and can typically be retained as-is.</p>
<p>When a dataset is released‚Äîeither publicly or to specific researchers‚Äîthe main risks are re-identification (linking data to an individual) and attribute disclosure (inferring confidential information about an identified person). These threats may arise from linkage attacks, in which an attacker combines anonymized data with external datasets, such as public voter registries or social media profiles, to re-establish identities. The well-known case of Latanya Sweeney‚Äôs re-identification of Massachusetts health records using public voter data illustrates this danger: even after names were removed, the combination of ZIP code, birth date, and sex uniquely identified a large percentage of individuals.</p>
<p>Structured data can appear in various forms depending on how it is shared. The simplest is tabular data, where aggregated statistics (like counts or averages) are published instead of individual records. While this reduces the risk of exposure, even aggregates can sometimes leak information when groups are small. Microdata releases contain individual-level records and pose higher risks but are more valuable for analysis. A third model involves queryable databases, where external users can run limited statistical queries without direct access to the data‚Äîan approach discussed more deeply in later sessions.</p>
<p>The core challenge of anonymization lies in finding the right balance between utility and privacy. If data is heavily modified or generalized, it may lose its usefulness for research or policy analysis; if it is insufficiently transformed, it remains vulnerable to privacy breaches. This balance can be described through two key measures: disclosure risk, the probability that an individual can be re-identified or a sensitive attribute inferred; and utility loss, the degree to which the anonymized data deviates from the original in ways that reduce its analytical value. In practice, anonymization often involves iterative processes that evaluate and adjust this trade-off.</p>
<p>Various approaches exist to anonymize structured data. A broad distinction can be made between non-perturbative and perturbative methods. Non-perturbative methods modify the structure of the data‚Äîby removing or grouping records, generalizing attribute values, or suppressing rare combinations‚Äîwithout changing the actual data values themselves. Examples include recoding numerical values into intervals (e.g., replacing exact ages with age ranges) or aggregating geographic details (replacing ‚Äústreet address‚Äù with ‚Äúpostal district‚Äù). Perturbative methods, on the other hand, introduce controlled randomness or noise to mask individual contributions while preserving global patterns. These include techniques like data swapping (exchanging values between records), noise addition (slightly altering numeric values), or synthetic data generation (creating artificial records that preserve statistical distributions).</p>
<p>To formalize privacy guarantees, researchers and practitioners use different models. The most widely known is k-anonymity, which requires that each record in a dataset be indistinguishable from at least k‚Äì1 others with respect to the quasi-identifiers. In other words, any given combination of identifying attributes must appear in at least k rows, making the probability of linking a record to a specific individual at most 1/k. Variants of this model, such as l-diversity and t-closeness, strengthen protection by ensuring diversity and similarity constraints within groups, reducing the risk of attribute disclosure even when sensitive attributes are unevenly distributed.</p>
<p>While these formal models are primarily applied to microdata, their principles also guide the anonymization of other data forms, including tabular and queryable data. In tabular data, cell suppression and controlled rounding are used to prevent inferences about small subgroups, while complementary suppression ensures that suppressed cells cannot be reconstructed from totals. When publishing frequency tables or census data, for instance, agencies routinely withhold or adjust values in small population groups to prevent re-identification.</p>
<p>An effective anonymization process typically begins with a detailed risk assessment. This includes identifying potential quasi-identifiers, estimating disclosure risk based on external data availability, and selecting appropriate transformation methods. After anonymization, utility evaluation ensures that the transformed data still supports the intended analyses‚Äîsuch as statistical inference, machine learning, or policy evaluation. Because anonymization is context-dependent, the same dataset might require different transformations depending on its intended use and audience: a public release requires stronger privacy, while restricted research access may allow more detail.</p>
<p>Ultimately, anonymization of structured databases illustrates a core theme in privacy engineering: privacy is not achieved by deleting data but by transforming it intelligently. It is a balance between protecting individuals and preserving collective knowledge. In practice, achieving full anonymity is nearly impossible‚Äîespecially as external data sources multiply‚Äîbut well-designed anonymization greatly reduces risk while maintaining social and scientific value.</p>
<section id="id1">
<h2>Privacy in Databases<a class="headerlink" href="#id1" title="Link to this heading">#</a></h2>
<p>An alternative strategy to protect data is to make them no longer linkable to individuals, that is, to anonymise them. Anonymised data are no longer considered personal, and thus the legal restrictions that apply to personal data are lifted. This
section describes the state of the art in data anonymisation techniques and models.</p>
<section id="respondent-privacy-statistical-disclosure-control">
<h3>Respondent Privacy: Statistical Disclosure Control<a class="headerlink" href="#respondent-privacy-statistical-disclosure-control" title="Link to this heading">#</a></h3>
<p>Traditionally, national statistical institutes and government agencies have systematically gathered information about individual respondents, either people or companies, with the aim of using it for policymaking and also distributing it for public and
private research that may benefit their country. The most detailed way to disseminate this information is by releasing a microdata set, essentially a database table,
each of whose records conveys information on a particular respondent. Although
these databases may be extremely useful to researchers, it is of fundamental importance that their publication does not compromise the respondents‚Äô privacy in the
sense of revealing information attributable to specific individuals. Statistical disclosure control (SDC) is the discipline that deals with the inherent trade-off between
protecting the privacy of the respondents and ensuring that the protected data are
still useful to researchers.
Usually, a microdata set contains a set of attributes that may be classified as
identifiers, key attributes (a.k.a. quasi-identifiers), or confidential attributes.
Identifiers allow unequivocal identification of individuals. Examples are social
security numbers or full names, which need to be removed before publication of the microdata set. On the other hand, key attributes are those attributes that, in combination, may allow linkage with external information to re-identify (some of) the
respondents to whom (some of) the records in the microdata set refer (identity disclosure). Examples include job, address, age, gender, height and weight. Last but
not least, the microdata set contains confidential attributes with sensitive information on respondents, such as salary, religion, political affiliation or health condition.
Beyond protecting against identity disclosure, SDC must prevent intruders from
guessing the confidential attribute values of specific respondents (attribute
disclosure).
Several SDC methods have been proposed in the literature to protect microdata
sets (Hundepool et al. 2012). Next, we briefly review the main ones.</p>
<section id="non-perturbative-masking">
<h4>Non-perturbative Masking<a class="headerlink" href="#non-perturbative-masking" title="Link to this heading">#</a></h4>
<p>In SDC, masking refers to the process of obtaining an anonymised data set X‚Äô by
modifying the original X. Masking can be perturbative or non-perturbative. In the
former approach, the data values of X are perturbed to obtain X‚Äô. In contrast, in nonperturbative masking X‚Äô is obtained by removing some values and/or by making
them more general; yet the information in X‚Äô is still true, although less detailed; as
an example, a value might be replaced by a range containing the original value.
Common non-perturbative methods include:</p>
<ul class="simple">
<li><p>Sampling. Instead of publishing the whole data set, only a sample of it is released.</p></li>
<li><p>Generalisation. The values of the different attributes are recoded in new, more
general categories such that the information remains the same, albeit less
specific.</p></li>
<li><p>Top/bottom coding. In line with the previous method, values above (resp. below)
a certain threshold are grouped together into a single category.</p></li>
<li><p>Local suppression. If a combination of quasi-identifier values is shared by too
few records, it may lead to re-identification. This method relies on replacing
certain individual attribute values with missing values, so that the number of
records sharing a particular combination of quasi-identifier values becomes
larger.</p></li>
</ul>
</section>
<section id="perturbative-masking">
<h4>Perturbative Masking<a class="headerlink" href="#perturbative-masking" title="Link to this heading">#</a></h4>
<p>Perturbative masking generates a modified version of the microdata set such that the
privacy of the respondents is protected to a certain extent while simultaneously
some statistical properties of the data are preserved. Well-known perturbative masking methods include:</p>
<ul class="simple">
<li><p>Noise addition. This is the most popular method, which consists in adding a
noise vector to each record in the data set. The utility preservation depends on the
amount and the distribution of the noise.</p></li>
<li><p>Data swapping. This technique exchanges the values of the attributes randomly
among individual records. Clearly, univariate distributions are preserved, but
multivariate distributions may be substantially harmed unless swaps of very different values are ruled out.</p></li>
<li><p>Microaggregation. This groups similar records together and releases the average
record of each group (Domingo-Ferrer and Mateo-Sanz 2002). The more similar
the records in a group, the more data utility is preserved.</p></li>
</ul>
</section>
<section id="synthetic-microdata-generation">
<h4>Synthetic Microdata Generation<a class="headerlink" href="#synthetic-microdata-generation" title="Link to this heading">#</a></h4>
<p>An anonymisation approach alternative to masking is synthetic data generation.
That is, instead of modifying the original data set, a simulated data set is generated
such that it preserves some properties of the original data set. The main advantage
of synthetic data is that no respondent re-identification seems possible since the data
are artificial. However, if, by chance, a synthetic record is very close to an original
one, the respondent of the latter record will not feel safe when the former record is
released. In addition, the utility of synthetic data sets is limited to preserving the
statistical properties selected at the time of data synthesis.
Some examples of synthetic generation include methods based on multiple
imputation (Rubin 1993) and methods that preserve means and co-variances
(Burridge 2003). An effective alternative to the drawbacks of purely synthetic data
are hybrid data, which mix original and synthetic data and are therefore more flexible (Domingo-Ferrer and Gonz√°lez-Nicol√°s 2010). Yet another alternative is partially synthetic data, whereby only the most sensitive original data values are
replaced by synthetic values.</p>
</section>
<section id="privacy-models">
<h4>Privacy Models<a class="headerlink" href="#privacy-models" title="Link to this heading">#</a></h4>
<p>For an anonymised data set X‚Äô to be safe/private enough, it needs to be sufficiently
anonymised. The level of anonymisation can be assessed after the generation of X‚Äô
or prior to it.
Ex post methods rely on the analysis of the output data set and, therefore, it is
possible to generate a data set that is not safe enough according to a certain criterion; several iterations with increasingly strict privacy parameters and decreasing
utility may be needed. The most commonly used ex post approach is masking followed by record linkage. Protection is sufficient high only if there is a sufficiently
low proportion of masked records that can be linked to the respective original
records they come from.</p>
<p>On the other hand, the ex ante approach relies on privacy models that allow
selecting the desired privacy level before producing X‚Äô. In this way, the output data
set is always as private as specified by the model, although it may fail to provide
enough utility if the model parameters are too strict.</p>
<section id="k-anonymity-and-extensions">
<h5>k-Anonymity and Extensions<a class="headerlink" href="#k-anonymity-and-extensions" title="Link to this heading">#</a></h5>
<p>A well-known privacy model is k-anonymity (Samarati and Sweeney 1998), which
requires that each tuple of key-attribute values be shared by at least k records in the
database. This condition may be achieved through generalisation and suppression
mechanisms, and also through microaggregation (Domingo-Ferrer and Torra 2005).
Unfortunately, while this privacy model prevents identity disclosure, it may fail
to protect against attribute disclosure. The definition of this privacy model establishes that complete re-identification is unfeasible within a group of records sharing
the same tuple of perturbed key-attribute values. However, if the records in the
group have the same value (or very similar values) for a confidential attribute, the
confidential attribute value of an individual linkable to the group is leaked.
To fix this problem, some extensions of k-anonymity have been proposed, the
most popular being l-diversity (Machanavajjhala et al. 2006) and t-closeness (Li
et al. 2007a). The property of l-diversity is satisfied if there are at least l ‚Äòwellrepresented‚Äô values for each confidential attribute in all groups sharing the values of
the quasi-identifiers. The property of t-closeness is satisfied when the distance
between the distribution of each confidential attribute within each group and the
whole data set is no more than a threshold t.</p>
</section>
<section id="differential-privacy">
<h5>Differential Privacy<a class="headerlink" href="#differential-privacy" title="Link to this heading">#</a></h5>
<p>Another important privacy model is differential privacy (Dwork 2006). This
model was originally defined for queryable databases and consists in perturbing
the original query result of a database before outputting it. This may be viewed as
equivalent to perturbing the original data and then computing the queries over the
modified data. Thus, differential privacy can also be seen as a privacy model for
microdata sets.
An Œµ-differentially private algorithm is one that, when run on two datasets that
differ in a single record, performs similarly (up to a power of Œµ) in both cases. That
is, the presence or the absence of any single record does not significantly alter the
output of the algorithm. Typically, Œµ-differential privacy is attained by adding
Laplace noise with zero mean and parameter Œî(f)/Œµ, where Œî(f) is the sensitivity of
the algorithm (the maximum change in the algorithm output that can be caused by a
change in a single record in the absence of noise) and Œµ is a privacy parameter; the
larger Œµ, the less privacy</p>
</section>
</section>
</section>
</section>
<section id="introduction">
<h2>Introduction<a class="headerlink" href="#introduction" title="Link to this heading">#</a></h2>
<p>In healthcare, data is often collected from patients to better understand disease patterns, improve treatments, and develop new medications.
Some common types of data that are often used in biomedical research include:
Demographic data: age, address, marital status, gender, etc.
Social data: education, income, etc.
Behavioral data: sleeping habits, smoker, drinker, drug user, etc.
Clinical data: diagnostics, medications, test results, etc.
Genomic data: genomic sequencing from samples.
Image data: x-rays, CT-scans, MRI, fMRI, etc.
Textual data: extra auxiliary information.</p>
<p>When processing, sharing, or releasing these data, we might do so as:
Tabular data. Tables with counts or magnitudes.
Queryable databases. On-line databases which accept statistical queries (sums, averages, max, min, etc.).
Microdata. Files where each record contains information on an individual (a physical person or an organization).</p>
<p>Statistical databases must provide useful statistical information.
They must also preserve the privacy of respondents if data are sensitive.
Anonymization measures aim to produce modified versions of the original datasets which are as close as possible to the original datasets but do not allow attackers to identify individuals or to infer confidential attributes about specific individuals.
Anonymous data are not classified as personal data and hence fall outside the scope of the GDPR.</p>
<p>Whichever the type and format of the data, the general aim of anonymization mechanisms is to minimize the risk of disclosure, in particular identity and/or attribute disclosure.
In addition to accidental leakages, such disclosures can occur because of attacks.
Record and attribute linkage attacks match auxiliary knowledge of an individual to records in private microdata sets to learn additional information about the target.
Membership and attribute inference leverage released aggregate data, such as statistical tables or machine learning models, to improve their probabilistic belief in some piece of information about a target individual.
Finally, reconstruction attacks try to reconstruct the original microdata (or training data in case of ML) from released statistics (or ML models).</p>
<p>Evaluation is in terms of two conflicting goals:
Minimize the data utility loss caused by the method.
Minimize the extant disclosure risk in the anonymized data.
The best methods are those that optimize the trade-off between both goals.
Two approaches exist to achieve this trade-off:
Utility-first anonymization: apply masking methods iteratively until a desired disclosure risk is met.
Privacy-first anonymization: use a privacy model, such as ùëò-anonymity or ùúñ-differential privacy, which sets a disclosure risk, to derive parameters for masking methods.</p>
</section>
<section id="tabular-data-protection">
<h2>Tabular data protection<a class="headerlink" href="#tabular-data-protection" title="Link to this heading">#</a></h2>
<p>Tabular data is one of the most common ways in which statistical information is published. Government agencies, research institutions, and companies often release aggregated tables summarizing results from surveys, censuses, or databases‚Äîfor example, the number of people by region and profession, or the average salary by sector. Although such tables appear harmless because they contain no individual records, they can still reveal sensitive information about individuals when analyzed carefully. Protecting privacy in tabular data publishing therefore requires specialized anonymization techniques that prevent both identity disclosure and attribute disclosure while maintaining the utility of the statistics.</p>
<p>A statistical table is usually constructed from microdata, meaning a collection of individual records that include multiple attributes for each person. When a table is built, those records are grouped according to certain key variables or quasi-identifiers (for example, region, age group, or gender), and a summary statistic‚Äîsuch as a count, total income, or average‚Äîis computed for each combination of categories. The resulting table thus contains cells, each representing a subgroup of individuals. For example, one cell might correspond to ‚ÄúWomen aged 40‚Äì49 working in Finance in Barcelona.‚Äù Even though only the count or average is shown, if that subgroup includes very few individuals, those numbers could reveal private details about them.</p>
<p>Privacy risks in tabular data publishing can arise in two main ways. The first is the external attack: when an outside party uses publicly available information or background knowledge to deduce information about an individual. For instance, if a table reports that there is exactly one person in a specific region and category, an attacker who knows someone matching those conditions can infer that person‚Äôs confidential attribute (such as salary or health status). The second is the internal attack, which can occur even among respondents within the dataset. If two people appear in the same cell and one of them knows their own data, they can infer the other‚Äôs by comparing with the published cell value (for example, if they know the average salary of a two-person group and their own salary, they can deduce the other‚Äôs).</p>
<p>Because these risks depend on the number of individuals contributing to each cell, the cell frequency‚Äîthe count of records behind each aggregate‚Äîis a crucial factor in tabular data protection. When cell frequencies are too small, the cell is considered unsafe. Statistical disclosure control (SDC) techniques aim to protect these cells while keeping the overall table as informative as possible.</p>
<p>The simplest protection strategy is cell suppression, where unsafe cells (those with very small counts or containing sensitive information) are removed or replaced with a symbol such as ‚Äú‚Äì‚Äù or ‚Äúconfidential.‚Äù However, suppression alone can be insufficient because totals and marginal sums may allow suppressed values to be recalculated. To prevent such deductive disclosure, complementary suppression is applied: additional, non-sensitive cells are also suppressed strategically so that the missing values cannot be reconstructed from the published totals. This ensures that every suppressed cell is supported by a set of other suppressed cells that make re-computation impossible.</p>
<p>Another method is controlled rounding, in which numerical values‚Äîsuch as counts or averages‚Äîare rounded to the nearest multiple of a specified base (for example, to the nearest 3 or 5). Rounding introduces small random errors that obscure exact cell frequencies while preserving global consistency and additivity of totals. Controlled tabular adjustment (CTA) is a related approach that makes minimal adjustments to cell values, usually through optimization algorithms, to ensure that sensitive cells deviate from their true values by at least a specified threshold while keeping the table as close as possible to the original.</p>
<p>In some cases, agencies publish interval data rather than exact values. Instead of reporting an exact count or average, they publish ranges such as ‚Äúbetween 3 and 7 individuals‚Äù or ‚Äúaverage income between ‚Ç¨35,000 and ‚Ç¨40,000.‚Äù This technique generalizes results and can be tuned to balance privacy and precision: wider intervals give stronger privacy at the cost of utility.</p>
<p>Noise addition can also be applied to tabular data, though it must be done carefully to preserve statistical consistency. Small random perturbations are introduced to cell values or to the input data before aggregation, ensuring that the overall distribution remains accurate for analysis but that individual contributions cannot be deduced. However, because perturbation introduces distortion, it must be calibrated to maintain the credibility of published statistics‚Äîespecially when the data serve policy or economic purposes.</p>
<p>To evaluate the effectiveness of these techniques, two criteria are commonly used: disclosure risk and information loss (or utility loss). Disclosure risk measures how likely it is that an individual‚Äôs information could be inferred from the published table. Information loss measures how much the modified table deviates from the original, affecting the usefulness of the data for analysis. An ideal anonymization strategy minimizes both‚Äîbut in practice, they are inversely related: increasing protection typically reduces accuracy and vice versa. Agencies therefore aim for a reasonable balance based on the sensitivity of the data and the intended use of the publication.</p>
<p>In the practical workflow of tabular data protection, the process typically begins with identifying key variables‚Äîthose attributes that could serve as quasi-identifiers‚Äîand determining a frequency threshold, such as three or five, below which a cell is considered unsafe. Then, one or more SDC methods (suppression, rounding, or perturbation) are applied. Finally, validation ensures that no suppressed or modified cells can be reconstructed through arithmetic relationships among totals or that noise addition does not excessively distort results.</p>
<p>An important aspect discussed in this session is that anonymizing tabular data is not the same as anonymizing microdata. In tabular data, individuals are already aggregated, so the primary goal is not to protect individual records directly but to prevent inference from aggregated statistics. The techniques, therefore, operate at the aggregate level‚Äîcontrolling how much information each published value reveals about the underlying data.</p>
<p>Ultimately, tabular data protection exemplifies one of the earliest and most classical forms of statistical disclosure control. It demonstrates the principle that even aggregate data can leak sensitive information if not handled carefully. By applying methods such as suppression, rounding, perturbation, and interval publication, data controllers can ensure that the results of statistical analyses remain useful for society while respecting the confidentiality of the individuals whose data underlie those numbers.</p>
<p>Goal: Publish static aggregate information, in such a way that no confidential information can be inferred on specific individuals to whom the table refers.
From microdata, tabular data can be generated by crossing one or more categorical attributes.
Formally, given categorical attributes <span class="math notranslate nohighlight">\(X_1, \cdots, X_l\)</span>, a table <span class="math notranslate nohighlight">\(T\)</span> is a function
$<span class="math notranslate nohighlight">\(T : D(X_1) \times D(X_2) \times \cdots \times D(X_l) \rightarrow R \; or \; N\)</span><span class="math notranslate nohighlight">\(
where \)</span>D(X_i)<span class="math notranslate nohighlight">\( is the domain where attribute \)</span>X_i$ takes its values.</p>
<section id="types-of-tables">
<h3>Types of tables<a class="headerlink" href="#types-of-tables" title="Link to this heading">#</a></h3>
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><p>id</p></th>
<th class="head"><p>Salary</p></th>
<th class="head"><p>Sector</p></th>
<th class="head"><p>Region</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>1</p></td>
<td><p>500000‚Ç¨</p></td>
<td><p>IT</p></td>
<td><p>BCN</p></td>
</tr>
<tr class="row-odd"><td><p>2</p></td>
<td><p>320000 ‚Ç¨</p></td>
<td><p>IT</p></td>
<td><p>BCN</p></td>
</tr>
<tr class="row-even"><td><p>3</p></td>
<td><p>32000 ‚Ç¨</p></td>
<td><p>IT</p></td>
<td><p>BCN</p></td>
</tr>
<tr class="row-odd"><td><p>4</p></td>
<td><p>35000 ‚Ç¨</p></td>
<td><p>IT</p></td>
<td><p>TGN</p></td>
</tr>
<tr class="row-even"><td><p>5</p></td>
<td><p>34000 ‚Ç¨</p></td>
<td><p>IT</p></td>
<td><p>TGN</p></td>
</tr>
<tr class="row-odd"><td><p>6</p></td>
<td><p>28000 ‚Ç¨</p></td>
<td><p>IT</p></td>
<td><p>TGN</p></td>
</tr>
<tr class="row-even"><td><p>7</p></td>
<td><p>300000 ‚Ç¨</p></td>
<td><p>HC</p></td>
<td><p>BCN</p></td>
</tr>
<tr class="row-odd"><td><p>8</p></td>
<td><p>45000 ‚Ç¨</p></td>
<td><p>HC</p></td>
<td><p>BCN</p></td>
</tr>
<tr class="row-even"><td><p>9</p></td>
<td><p>34000 ‚Ç¨</p></td>
<td><p>HC</p></td>
<td><p>BCN</p></td>
</tr>
<tr class="row-odd"><td><p>10</p></td>
<td><p>45000 ‚Ç¨</p></td>
<td><p>HC</p></td>
<td><p>TGN</p></td>
</tr>
<tr class="row-even"><td><p>11</p></td>
<td><p>34000 ‚Ç¨</p></td>
<td><p>HC</p></td>
<td><p>TGN</p></td>
</tr>
<tr class="row-odd"><td><p>12</p></td>
<td><p>24000 ‚Ç¨</p></td>
<td><p>HC</p></td>
<td><p>TGN</p></td>
</tr>
<tr class="row-even"><td><p>13</p></td>
<td><p>300000 ‚Ç¨</p></td>
<td><p>Fin</p></td>
<td><p>BCN</p></td>
</tr>
<tr class="row-odd"><td><p>14</p></td>
<td><p>350000 ‚Ç¨</p></td>
<td><p>Fin</p></td>
<td><p>BCN</p></td>
</tr>
<tr class="row-even"><td><p>15</p></td>
<td><p>150000 ‚Ç¨</p></td>
<td><p>Fin</p></td>
<td><p>TGN</p></td>
</tr>
</tbody>
</table>
</div>
<table>
    <tr>
        <th> Frequency </th>
        <th style="text-align: center" colspan=3> Region </th>
    </tr>
    <tr>
        <th> <b>Sector</b> </th>
        <th> BCN </th>
        <th> TGN </th>
        <th> Total </th>
    </tr>
        <td> IT </td>
        <td> 3 </td>
        <td> 3 </td>
        <td> 6 </td>
    <tr>
        <td> Fin </td>
        <td> 2 </td>
        <td> 1 </td>
        <td> 3 </td>
    </tr>
    <tr>
        <td> HC </td>
        <td> 3 </td>
        <td> 3 </td>
        <td> 6 </td>
    </tr>
    <tr>
        <td> Total </td>
        <td> 8 </td>
        <td> 7 </td>
        <td> 15 </td>
    </tr>
</table>
<table>
    <tr>
        <th> Sum of Salary </th>
        <th style="text-align: center" colspan=3> Region </th>
    </tr>
    <tr>
        <th> <b>Sector</b> </th>
        <th> BCN </th>
        <th> TGN </th>
        <th> Total </th>
    </tr>
        <td> IT </td>
        <td> 852000 ‚Ç¨ </td>
        <td> 97000 ‚Ç¨ </td>
        <td> 949000 ‚Ç¨ </td>
    <tr>
        <td> Fin </td>
        <td> 335000 ‚Ç¨ </td>
        <td> 150000 ‚Ç¨ </td>
        <td> 485000 ‚Ç¨ </td>
    </tr>
    <tr>
        <td> HC </td>
        <td> 379000 ‚Ç¨ </td>
        <td> 103000 ‚Ç¨ </td>
        <td> 482000 ‚Ç¨ </td>
    </tr>
    <tr>
        <td> Total </td>
        <td> 1566000 ‚Ç¨ </td>
        <td> 350000 ‚Ç¨ </td>
        <td> 1916000 ‚Ç¨ </td>
    </tr>
</table>
</section>
<section id="disclosure-attacks-in-tables">
<h3>Disclosure attacks in tables<a class="headerlink" href="#disclosure-attacks-in-tables" title="Link to this heading">#</a></h3>
<p>External attack
Let a released frequency table <strong>Ethnicity</strong> <span class="math notranslate nohighlight">\(\times\)</span> <strong>Town</strong> contain a single respondent for ethnicity <span class="math notranslate nohighlight">\(E_i\)</span> and town <span class="math notranslate nohighlight">\(T_j\)</span>.
If a magnitude table is released with the average blood pressure for each ethnicity and each town, the exact blood pressure of the only respondent with ethnicity <span class="math notranslate nohighlight">\(ùê∏_ùëñ\)</span> in town <span class="math notranslate nohighlight">\(ùëá_ùëó\)</span> is publicly disclosed.
Internal attack
If there are only two respondents for ethnicity <span class="math notranslate nohighlight">\(ùê∏_ùëñ\)</span> and town <span class="math notranslate nohighlight">\(ùëá_ùëó\)</span> , the blood pressure of each of them is disclosed to the other.
Dominance attack
If one (or few) respondents dominate in the contribution to a cell in a magnitude table, the dominant respondent(s) can upper-bound the contributions of the rest.
<em>E.g.</em>, if the table displays the cumulative earnings for each job type and town, and one individual contributes 90% of a certain cell value, they know their colleagues in the town are not doing very well.</p>
</section>
<section id="sdc-methods-for-tables">
<h3>SDC Methods for tables<a class="headerlink" href="#sdc-methods-for-tables" title="Link to this heading">#</a></h3>
<p>Non-perturbative. They do not modify the values in the cells, but they may suppress or recode them.
Best known methods:
cell suppression (CS).
recoding of categorical attributes.
Perturbative. They modify the values in the cells.
Best known methods:
controlled rounding (CR).
controlled tabular  adjustment (CTA).</p>
<section id="cell-suppression">
<h4>Cell suppression<a class="headerlink" href="#cell-suppression" title="Link to this heading">#</a></h4>
<p>Identify sensitive cells, using a sensitivity rule.
Suppress values in sensitive cells (primary suppressions).
Perform additional suppressions (secondary suppressions) to prevent recovery of primary suppressions from row and/or column marginals.</p>
</section>
<section id="sensitivity-rules">
<h4>Sensitivity rules<a class="headerlink" href="#sensitivity-rules" title="Link to this heading">#</a></h4>
<p>Minimum frequency rule: the cell frequency is less than a pre-specified minimum frequency ùëõ.
(ùíè,ùíå)-dominance rule: the sum of the ùëõ largest contributors exceeds ùëò% of the cell total.
ùíë%-rule: the cell total minus the 2 biggest contributors is less than ùëù% of the largest contribution.
ùíëùíí-rule: Generalization of the ùëù%-rule, which takes into account prior information. The 2nd biggest contributor can estimate the rest of contributions within ùëû%.</p>
<table>
    <tr>
        <th> Sum of Salary </th>
        <th style="text-align: center" colspan=3> Region </th>
    </tr>
    <tr>
        <th> <b>Sector</b> </th>
        <th> BCN </th>
        <th> TGN </th>
        <th> Total </th>
    </tr>
        <td> IT </td>
        <td style="color:#FF0000"> 852000 ‚Ç¨ </td>
        <td> 97000 ‚Ç¨ </td>
        <td> 949000 ‚Ç¨ </td>
    <tr>
        <td> Fin </td>
        <td style="color:#FF0000"> 335000 ‚Ç¨ </td>
        <td style="color:#FF0000"> 150000 ‚Ç¨ </td>
        <td> 485000 ‚Ç¨ </td>
    </tr>
    <tr>
        <td> HC </td>
        <td> 379000 ‚Ç¨ </td>
        <td> 103000 ‚Ç¨ </td>
        <td> 482000 ‚Ç¨ </td>
    </tr>
    <tr>
        <td> Total </td>
        <td> 1566000 ‚Ç¨ </td>
        <td> 350000 ‚Ç¨ </td>
        <td> 1916000 ‚Ç¨ </td>
    </tr>
</table>
</section>
<section id="secondary-suppressions">
<h4>Secondary suppressions<a class="headerlink" href="#secondary-suppressions" title="Link to this heading">#</a></h4>
<p>Usually, one attempts to minimize either the number of secondary suppressions or their marginals.
Optimization methods are heuristic, based on mixed linear integer programming or networks flows (the latter for 2-D tables only).</p>
<table>
    <tr>
        <th> Sum of Salary </th>
        <th style="text-align: center" colspan=3> Region </th>
    </tr>
    <tr>
        <th> <b>Sector</b> </th>
        <th> BCN </th>
        <th> TGN </th>
        <th> Total </th>
    </tr>
        <td> IT </td>
        <td> - </td>
        <td> 97000 ‚Ç¨ </td>
        <td> 949000 ‚Ç¨ </td>
    <tr>
        <td> Fin </td>
        <td> - </td>
        <td> - </td>
        <td> 485000 ‚Ç¨ </td>
    </tr>
    <tr>
        <td> HC </td>
        <td> 379000 ‚Ç¨ </td>
        <td> 103000 ‚Ç¨ </td>
        <td> 482000 ‚Ç¨ </td>
    </tr>
    <tr>
        <td> Total </td>
        <td> 1566000 ‚Ç¨ </td>
        <td> 350000 ‚Ç¨ </td>
        <td> 1916000 ‚Ç¨ </td>
    </tr>
</table>
<p>A Feasibility Interval is the range of possible values that a suppressed cell can take, given the linear relations between cells and the row and column totals.
In here, <span class="math notranslate nohighlight">\(X_{11}\)</span> must be in range <span class="math notranslate nohighlight">\([3,6]\)</span>, because
<span class="math notranslate nohighlight">\(ùëã_{21}+ùëã_{22}= 3 \rightarrow ùëã_{21} \le 3\)</span>,
<span class="math notranslate nohighlight">\(ùëã_{11}+ùëã_{21}+3=9 \rightarrow ùëã_{11} \le 6\)</span>, and
<span class="math notranslate nohighlight">\(ùëã_{11}+ùëã_{21}+3=9  \rightarrow ùëã_{11} \ge 3\)</span>.</p>
<table>
    <tr>
        <th> Example </th>
        <th style="text-align: center" colspan=3> Attribute 1 </th>
    </tr>
    <tr>
        <th> <b>Attribute 2</b> </th>
        <th> 1 </th>
        <th> 2 </th>
        <th> Total </th>
    </tr>
        <td> 1 </td>
        <td> X11 </td>
        <td> X12 </td>
        <td> 7</td>
    <tr>
        <td> 2 </td>
        <td> X21 </td>
        <td> X22 </td>
        <td> 3 </td>
    </tr>
    <tr>
        <td> 3 </td>
        <td> 3 </td>
        <td> 3 </td>
        <td> 6 </td>
    </tr>
    <tr>
        <td> Total </td>
        <td> 9 </td>
        <td> 7 </td>
        <td> 16 </td>
    </tr>
</table>
<p>The feasibility interval must contain the protection interval.
The protection interval is computed according to the chosen sensitivity rule <span class="math notranslate nohighlight">\([ùê∂ùëâ‚àíùëÉùêø, ùê∂ùëâ+ùëÉùêø]\)</span>.</p>
<p><img alt="alt text" src="_images/protection_levels.png" /></p>
</section>
<section id="controlled-rounding-and-controlled-tabular-adjustment">
<h4>Controlled rounding and Controlled tabular adjustment<a class="headerlink" href="#controlled-rounding-and-controlled-tabular-adjustment" title="Link to this heading">#</a></h4>
<p>CR rounds values in the table to multiples of a rounding base (marginals may have to be rounded as well).
CTA modifies the values in the table to prevent inference of sensitive cell values within a prescribed protection interval.
CTA attempts to find the closest table to the original one that protects all sensitive cells.
CTA optimization is typically based on mixed linear integer programming and entails less information loss than CS.</p>
</section>
</section>
<section id="utility-loss-in-tabular-sdc">
<h3>Utility loss in tabular SDC<a class="headerlink" href="#utility-loss-in-tabular-sdc" title="Link to this heading">#</a></h3>
<p>For cell suppression, utility loss is measured as the number of secondary suppressions or their pooled magnitude.
For controlled tabular adjustment or rounding, it is measured as the sum of distances between true and perturbed cell values.
The above loss measures may be weighted by cell costs, if not all cells have the same importance.</p>
</section>
<section id="disclosure-risk-in-tabular-sdc">
<h3>Disclosure risk in tabular SDC<a class="headerlink" href="#disclosure-risk-in-tabular-sdc" title="Link to this heading">#</a></h3>
<p>Disclosure risk is evaluated by computing the feasibility intervals for sensitive cells (via linear programming constrained by the marginals).
What could be the range of values of the suppressed cells given other cells, including marginals?
The table is safe if the feasibility interval for any sensitive cell contains the protection interval defined for that cell.
If a cell was suppressed because of the ùëù%-rule, does this ùëù% lie within the feasibility interval?
The protection interval is the narrowest interval estimate of the sensitive cell permitted by the data protector.</p>
</section>
</section>
<section id="interactive-databases">
<h2>Interactive databases<a class="headerlink" href="#interactive-databases" title="Link to this heading">#</a></h2>
<p>After discussing the protection of tabular data, which focuses on publishing static, aggregated statistics, this session turns to the more dynamic scenario of queryable databases‚Äîsystems that do not publish raw or aggregated data directly but instead allow users to make statistical queries through controlled interfaces, such as APIs or query engines. These databases are increasingly common in government, research, and commercial environments because they promise flexibility: external analysts or applications can request statistics or summaries without the data ever leaving the controller‚Äôs server. However, this model still poses serious privacy challenges. Even when individual records are not exposed, the answers to queries can leak sensitive information, especially if users issue cleverly designed or repeated queries.</p>
<p>A queryable database typically consists of three entities:</p>
<p>A data controller or curator, who collects and stores data from respondents or participants (for example, through surveys, services, or sensors).</p>
<p>A dataset, often containing personal information, organized in relational form.</p>
<p>A user, who can submit statistical queries‚Äîsuch as counts, sums, or averages‚Äîvia an interface, often programmatically.</p>
<p>The central privacy objective is to protect the respondents, not the users. That is, the system must ensure that queries cannot reveal confidential information about individuals whose data is stored in the database.</p>
<p>Privacy Risks in Queryable Databases</p>
<p>Even seemingly harmless queries can compromise privacy when their conditions isolate small subsets of individuals. For instance, if a query asks, ‚ÄúHow many employees in the finance sector in Tarragona earn more than ‚Ç¨100,000?‚Äù and the answer is ‚Äú1,‚Äù an attacker might infer both the existence and the salary of that single individual. If follow-up queries refine conditions (for example, by changing filters or combining results), they can extract additional details. This risk increases when users collude or use external information to link responses with real-world identities.</p>
<p>Two main forms of disclosure can occur:</p>
<p>Identity disclosure, when a query indirectly reveals who an individual is.</p>
<p>Attribute disclosure, when the value of a confidential attribute (such as salary or diagnosis) can be inferred for an identified person.</p>
<p>Unlike static table publication, protecting a queryable database is an ongoing process: each new query potentially leaks more information. Thus, privacy protection mechanisms must monitor query patterns over time and enforce limits dynamically.</p>
<p>Three Families of Protection Techniques</p>
<p>Three broad strategies are used to protect queryable databases: query restriction, query camouflage (generalization), and query perturbation. Each offers a different balance between privacy, computational cost, and result accuracy.</p>
<p>Query Restriction
The most straightforward method is to simply refuse to answer queries that could compromise privacy. The system computes the query set size‚Äîthe number of individual records that match the query conditions‚Äîand denies responses when that number falls below a predefined threshold. For example, the system might only answer queries that affect at least three or five individuals. This prevents small-group disclosures and is analogous to cell suppression in tabular data.</p>
<p>The advantage of query restriction is that it preserves exact accuracy: all returned results are true and unmodified. However, it is extremely demanding to implement securely. The curator must not only monitor each query but also consider query history‚Äîbecause multiple queries can be combined to extract sensitive information even if each, individually, is safe. Keeping track of overlaps between query sets (and potential collusion between users) is computationally expensive, especially for large datasets or open systems with many users. Therefore, while conceptually simple, query restriction scales poorly in practice.</p>
<p>Query Camouflage (Interval or Generalized Responses)
Another strategy is to generalize the answers by reporting intervals instead of precise values. For example, instead of returning that the average salary is ‚Ç¨42,530, the system might report that it lies between ‚Ç¨40,000 and ‚Ç¨45,000. This method provides bounded uncertainty without adding random noise, which preserves logical consistency and allows unlimited querying.</p>
<p>Each confidential attribute can have its own privacy bound‚Äîa parameter defining how wide the intervals should be. A larger interval offers stronger privacy but reduces the utility of the data, since results become less specific. Because interval reporting does not distort the data but rather abstracts it, it is considered a non-perturbative method. However, as with other generalization approaches, it cannot fully prevent inference when users accumulate many queries with overlapping conditions.</p>
<p>Query Perturbation (Noise Addition)
The most powerful and conceptually advanced approach is to add controlled noise to query responses so that individual contributions are hidden, while overall statistical patterns remain accurate. Noise can be introduced at two stages:</p>
<p>Input perturbation, where random noise is added to the raw data before any queries are processed.</p>
<p>Output perturbation, where noise is added to the query result itself before returning it to the user.</p>
<p>Input perturbation provides consistent privacy guarantees across queries but permanently modifies the dataset. Output perturbation preserves the dataset but requires generating new noise for each query, raising challenges for consistency and cumulative leakage.</p>
<p>Differential Privacy</p>
<p>The theoretical foundation for query perturbation is differential privacy (DP)‚Äîa mathematically rigorous model introduced in the mid-2000s and now widely used in both research and industry. Differential privacy formalizes the idea that the inclusion or exclusion of a single individual‚Äôs data should not significantly affect the result of any analysis.</p>
<p>More precisely, a randomized algorithm satisfies differential privacy if, for any two datasets that differ in only one individual, the probability of obtaining a particular output is nearly the same. The difference between these probabilities is controlled by a parameter called epsilon (Œµ), the privacy budget. A smaller Œµ provides stronger privacy (less sensitivity to any individual record) but introduces more noise and therefore reduces accuracy.</p>
<p>In practical terms, differential privacy gives a quantifiable, provable privacy guarantee that does not depend on what external information attackers might possess. It limits what can be inferred about any individual, even in the worst-case scenario. This makes it fundamentally stronger than heuristic approaches like suppression or generalization.</p>
<p>The implementation of differential privacy usually involves adding random noise drawn from a specific probability distribution‚Äîmost commonly Laplace or Gaussian‚Äîto query outputs. The amount of noise depends on the sensitivity of the query function (how much a single individual‚Äôs data can change the result) and on the chosen privacy parameter Œµ. For example, a simple count query has sensitivity 1 (each individual can change the count by at most one), whereas averages or sums have higher sensitivity.</p>
<p>Differential privacy also introduces the concept of a privacy budget over multiple queries. Each query consumes part of the available Œµ; when the total budget is exhausted, no further queries can be answered without violating the overall privacy guarantee. This ensures that repeated querying cannot gradually erode privacy protection.</p>
<p>This model has become the de facto standard for privacy-preserving analytics. It has been adopted in high-profile deployments such as the U.S. Census Bureau‚Äôs 2020 census, which used differential privacy to protect published statistics, and by major technology companies like Apple, Google, and Meta for data collection and analysis of user behavior.</p>
<p>The Randomized Response Analogy</p>
<p>To make the intuition behind differential privacy more accessible, the lecture introduced an older statistical method called randomized response, developed in the 1960s for social science surveys. In randomized response, participants answer sensitive yes/no questions (for example, ‚ÄúHave you ever used illegal drugs?‚Äù) using a randomizing device such as a coin flip. If the coin lands heads, the respondent answers truthfully; if tails, they answer ‚Äúyes‚Äù regardless of the truth. Because the interviewer cannot know whether any given answer is truthful, the individual‚Äôs privacy is protected. Yet, because the probability of randomness is known, the overall proportion of ‚Äúyes‚Äù answers in the population can still be estimated statistically.</p>
<p>Differential privacy generalizes this idea to databases: by adding controlled randomness to query results, it protects individuals‚Äô data while still allowing accurate aggregate statistics to be derived.</p>
<p>Conclusion</p>
<p>Queryable databases illustrate a fundamental trade-off between data utility and individual privacy. Restriction methods guarantee perfect accuracy but are computationally and operationally costly. Generalization provides deterministic uncertainty but can still be circumvented through repeated queries. Noise addition‚Äîespecially as formalized by differential privacy‚Äîoffers strong, quantifiable protection with manageable utility loss, making it the most promising modern approach.</p>
<p>This model marks a conceptual shift from heuristic anonymization toward mathematically provable privacy. Rather than focusing on hiding or deleting specific identifiers, differential privacy ensures that the participation of any individual has an almost negligible effect on analytical outcomes. It is, therefore, the cornerstone of contemporary privacy-preserving data analytics and the natural evolution of statistical disclosure control in the age of big data and machine learning</p>
<p><img alt="alt text" src="_images/interactive_db.png" /></p>
<section id="issues">
<h3>Issues<a class="headerlink" href="#issues" title="Link to this heading">#</a></h3>
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><p>id</p></th>
<th class="head"><p>Salary</p></th>
<th class="head"><p>Sector</p></th>
<th class="head"><p>Region</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>1</p></td>
<td><p>500000‚Ç¨</p></td>
<td><p>IT</p></td>
<td><p>BCN</p></td>
</tr>
<tr class="row-odd"><td><p>2</p></td>
<td><p>320000 ‚Ç¨</p></td>
<td><p>IT</p></td>
<td><p>BCN</p></td>
</tr>
<tr class="row-even"><td><p>3</p></td>
<td><p>32000 ‚Ç¨</p></td>
<td><p>IT</p></td>
<td><p>BCN</p></td>
</tr>
<tr class="row-odd"><td><p>4</p></td>
<td><p>35000 ‚Ç¨</p></td>
<td><p>IT</p></td>
<td><p>TGN</p></td>
</tr>
<tr class="row-even"><td><p>5</p></td>
<td><p>34000 ‚Ç¨</p></td>
<td><p>IT</p></td>
<td><p>TGN</p></td>
</tr>
<tr class="row-odd"><td><p>6</p></td>
<td><p>28000 ‚Ç¨</p></td>
<td><p>IT</p></td>
<td><p>TGN</p></td>
</tr>
<tr class="row-even"><td><p>7</p></td>
<td><p>300000 ‚Ç¨</p></td>
<td><p>HC</p></td>
<td><p>BCN</p></td>
</tr>
<tr class="row-odd"><td><p>8</p></td>
<td><p>45000 ‚Ç¨</p></td>
<td><p>HC</p></td>
<td><p>BCN</p></td>
</tr>
<tr class="row-even"><td><p>9</p></td>
<td><p>34000 ‚Ç¨</p></td>
<td><p>HC</p></td>
<td><p>BCN</p></td>
</tr>
<tr class="row-odd"><td><p>10</p></td>
<td><p>45000 ‚Ç¨</p></td>
<td><p>HC</p></td>
<td><p>TGN</p></td>
</tr>
<tr class="row-even"><td><p>11</p></td>
<td><p>34000 ‚Ç¨</p></td>
<td><p>HC</p></td>
<td><p>TGN</p></td>
</tr>
<tr class="row-odd"><td><p>12</p></td>
<td><p>24000 ‚Ç¨</p></td>
<td><p>HC</p></td>
<td><p>TGN</p></td>
</tr>
<tr class="row-even"><td><p>13</p></td>
<td><p>300000 ‚Ç¨</p></td>
<td><p>Fin</p></td>
<td><p>BCN</p></td>
</tr>
<tr class="row-odd"><td><p>14</p></td>
<td><p>350000 ‚Ç¨</p></td>
<td><p>Fin</p></td>
<td><p>BCN</p></td>
</tr>
<tr class="row-even"><td><p>15</p></td>
<td><p>150000 ‚Ç¨</p></td>
<td><p>Fin</p></td>
<td><p>TGN</p></td>
</tr>
</tbody>
</table>
</div>
<div class="highlight-sql notranslate"><div class="highlight"><pre><span></span><span class="k">SELECT</span><span class="w"> </span><span class="k">count</span><span class="p">()</span><span class="w"> </span><span class="k">FROM</span><span class="w"> </span><span class="k">table</span><span class="w"> </span><span class="k">WHERE</span><span class="w"> </span><span class="n">Salary</span><span class="w"> </span><span class="o">&gt;</span><span class="w"> </span><span class="mi">100</span><span class="p">.</span><span class="mi">000</span>
<span class="k">Result</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">5</span>
<span class="k">SELECT</span><span class="w"> </span><span class="k">sum</span><span class="p">(</span><span class="n">Salary</span><span class="p">)</span><span class="w"> </span><span class="k">FROM</span><span class="w"> </span><span class="k">table</span><span class="w"> </span><span class="k">WHERE</span><span class="w"> </span><span class="n">Sector</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">HC</span>
<span class="k">Result</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">482</span><span class="p">.</span><span class="mi">000</span>
<span class="k">SELECT</span><span class="w"> </span><span class="k">count</span><span class="p">()</span><span class="w"> </span><span class="k">FROM</span><span class="w"> </span><span class="k">table</span><span class="w"> </span><span class="k">WHERE</span><span class="w"> </span><span class="n">Sector</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">IT</span><span class="w"> </span><span class="k">and</span><span class="w"> </span><span class="n">Region</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">TGN</span>
<span class="k">Result</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">3</span>
<span class="k">SELECT</span><span class="w"> </span><span class="k">count</span><span class="p">()</span><span class="w"> </span><span class="k">FROM</span><span class="w"> </span><span class="k">table</span><span class="w"> </span><span class="k">WHERE</span><span class="w"> </span><span class="n">Sector</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">Fin</span><span class="w"> </span><span class="k">and</span><span class="w"> </span><span class="n">Region</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">TGN</span>
<span class="k">Result</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">1</span>
<span class="k">SELECT</span><span class="w"> </span><span class="k">avg</span><span class="p">(</span><span class="n">Salary</span><span class="p">)</span><span class="w"> </span><span class="k">FROM</span><span class="w"> </span><span class="k">table</span><span class="w"> </span><span class="k">WHERE</span><span class="w"> </span><span class="n">Sector</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">Fin</span><span class="w"> </span><span class="k">and</span><span class="w"> </span><span class="n">Region</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">TGN</span>
<span class="k">Result</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">150</span><span class="p">.</span><span class="mi">000</span>
</pre></div>
</div>
</section>
<section id="interactive-database-protection">
<h3>Interactive database protection<a class="headerlink" href="#interactive-database-protection" title="Link to this heading">#</a></h3>
<p>Three main SDC approaches:
Query restriction. The database refuses to answer certain queries.
Camouflage. Deterministically correct non-exact answers (small interval answers) are returned by the database.
Query perturbation. Perturbation (noise addition) can be applied to the microdata records on which queries are computed (input perturbation) or to the query result after computing it on the original data (output perturbation).</p>
<section id="query-restriction">
<h4>Query restriction<a class="headerlink" href="#query-restriction" title="Link to this heading">#</a></h4>
<p>This is the right approach if the user does require deterministically correct answers and these answers must be exact (i.e., a number).
Exact answers may be very disclosive, so it may be necessary to refuse answering certain queries at some stage.
A common criterion to decide whether a query can be answered is query set size control: the answer to a query is refused if this query together with the previously answered ones isolates too small a set of records.
Problems: computational burden to keep track of previous queries, collusion possible.</p>
</section>
<section id="camouflage">
<h4>Camouflage<a class="headerlink" href="#camouflage" title="Link to this heading">#</a></h4>
<p>Interval answers are returned rather than point answers.
No distortion of answers.
Unlimited answers can be returned.
Each query can only concern a single confidential attribute.
Privacy bounds are set for each confidential attribute.
The data curator can set a global bound, e.g., 30% (ùë¢_ùëñ‚àí‚Ñì_ùëñ‚â•0.3ùë•_ùëñ).
Respondents may request specific upper and lower bounds for their values.</p>
</section>
<section id="query-perturbation">
<h4>Query perturbation<a class="headerlink" href="#query-perturbation" title="Link to this heading">#</a></h4>
<p>Add noise to the values of confidential attributes in the original microdata (input perturbation).
We will cover that when we talk about microdata protection.
Add noise to the results of the queries after computing them on the original data (output perturbation).
Under the differential privacy model, we can give theoretical guarantees on the privacy level achieved with output perturbation.</p>
</section>
<section id="randomized-response">
<h4>Randomized response<a class="headerlink" href="#randomized-response" title="Link to this heading">#</a></h4>
<p>There are <span class="math notranslate nohighlight">\(n\)</span> people, and individual <span class="math notranslate nohighlight">\(i\)</span> has a sensitive bit <span class="math notranslate nohighlight">\(X_i \in \{0,1\}\)</span>.
Each person sends the analyst a message <span class="math notranslate nohighlight">\(Y_i\)</span>, which may depend on <span class="math notranslate nohighlight">\(X_i\)</span> and some random numbers which the individual can generate.
Based on these <span class="math notranslate nohighlight">\(Y_i\)</span>, the analyst would like to estimate <span class="math notranslate nohighlight">\(p=1/n \sum_i X_i\)</span>.
Consider the following strategy, which we will call Randomized Response, parameterized by some <span class="math notranslate nohighlight">\(\gamma\in [0, 1/2]\)</span>.
<span class="math notranslate nohighlight">\(Y_i = X_i\)</span> with probability <span class="math notranslate nohighlight">\(1/2 + \gamma\)</span> and <span class="math notranslate nohighlight">\(Y_i = 1-X_i\)</span> with probability <span class="math notranslate nohighlight">\(1/2-\gamma\)</span>.
How private <span class="math notranslate nohighlight">\(Y_i\)</span> and how accurate will the estimation of <span class="math notranslate nohighlight">\(y\)</span>? It depends on <span class="math notranslate nohighlight">\(\gamma\)</span>. There is plausible deniability.</p>
</section>
<section id="id2">
<h4>Differential privacy<a class="headerlink" href="#id2" title="Link to this heading">#</a></h4>
<p>We imagine there are <span class="math notranslate nohighlight">\(n\)</span> individuals, <span class="math notranslate nohighlight">\(X_1\)</span> through <span class="math notranslate nohighlight">\(X_n\)</span>, who each have their own datapoint.
They send this point to a ‚Äútrusted curator‚Äù.
Given their data, the curator runs an algorithm <span class="math notranslate nohighlight">\(M\)</span>, and publicly outputs the result of this computation.
Differential privacy is a property of <span class="math notranslate nohighlight">\(M\)</span> saying that no individual‚Äôs data has a large impact on the output.
Suppose we have an algorithm <span class="math notranslate nohighlight">\(M:X^n \rightarrow Y\)</span>. Consider any two datasets <span class="math notranslate nohighlight">\(X, X' \in X^n\)</span> which differ in exactly one entry. We call these neighboring datasets. We say that <span class="math notranslate nohighlight">\(M\)</span> is <span class="math notranslate nohighlight">\(\epsilon\)</span>-differentially private (<span class="math notranslate nohighlight">\(\epsilon\)</span>-DP) if, for all neighboring <span class="math notranslate nohighlight">\(X, X'\)</span>, and all <span class="math notranslate nohighlight">\(T \subseteq Y\)</span>, we have
$<span class="math notranslate nohighlight">\(Pr‚Å°[M(X) \in T] \le e^\epsilon Pr‚Å°[M(X') \in T]\)</span><span class="math notranslate nohighlight">\(
where the randomness is over the choices made by \)</span>M$.</p>
<p>The sensitivity of a function <span class="math notranslate nohighlight">\(\Delta f\)</span> (or <span class="math notranslate nohighlight">\(\Delta\)</span> if <span class="math notranslate nohighlight">\(f\)</span> is known) is the maximum distance between the results of <span class="math notranslate nohighlight">\(f\)</span> computed on neighboring datasets.
This can be defined as the <span class="math notranslate nohighlight">\(\ell_1\)</span> distance between results of <span class="math notranslate nohighlight">\(M\)</span> on neighboring datasets (<span class="math notranslate nohighlight">\((\epsilon,\delta)\)</span>-DP requires the <span class="math notranslate nohighlight">\(\ell_2\)</span> distance).
For example:</p>
<ul class="simple">
<li><p>Count queries have a <span class="math notranslate nohighlight">\(\Delta\)</span> of 1.</p></li>
<li><p>Average queries have a <span class="math notranslate nohighlight">\(\Delta\)</span> of <span class="math notranslate nohighlight">\(\max/n\)</span>.</p></li>
<li><p>Sum queries have a <span class="math notranslate nohighlight">\(\Delta\)</span> of <span class="math notranslate nohighlight">\(\max\)</span>.</p></li>
</ul>
<section id="the-laplace-mechanism">
<h5>The Laplace mechanism<a class="headerlink" href="#the-laplace-mechanism" title="Link to this heading">#</a></h5>
<p>The Laplace mechanism is a fundamental technique for achieving differential privacy.
Given a function <span class="math notranslate nohighlight">\(f: D \rightarrow R^d, where \)</span>D<span class="math notranslate nohighlight">\( is the domain of the dataset and \)</span>d<span class="math notranslate nohighlight">\( is the dimension of the output, the Laplace mechanism \)</span>M_{Laplace}<span class="math notranslate nohighlight">\( adds Laplace noise to the output of \)</span>f<span class="math notranslate nohighlight">\(. That is \)</span>M_{Laplace}(D)=f(D)+Lap(\Delta/\epsilon)<span class="math notranslate nohighlight">\( 
Let \)</span>b<span class="math notranslate nohighlight">\( be the scale parameter of the Laplace distribution, which is given by:
\)</span><span class="math notranslate nohighlight">\(Lap(b)=\frac{1}{2b} e^{-\frac{|x|}{b}}\)</span>$</p>
<p><img alt="alt text" src="_images/laplace.png" /></p>
</section>
<section id="properties-of-dp">
<h5>Properties of DP<a class="headerlink" href="#properties-of-dp" title="Link to this heading">#</a></h5>
<p><span class="math notranslate nohighlight">\(\epsilon\)</span> (privacy budget) should be small. Ideally in the range <span class="math notranslate nohighlight">\((0,1]\)</span>.
Resistance to post-processing. Once a quantity is privatized, it can‚Äôt be ‚Äúun-privatized,‚Äù if the data is not used again.
Group differential privacy. When neighboring datasets differ in <span class="math notranslate nohighlight">\(k\)</span> positions, the guarantee goes to <span class="math notranslate nohighlight">\(k\epsilon-DP\)</span>.
Composition rules. If we run <span class="math notranslate nohighlight">\(k\)</span> <span class="math notranslate nohighlight">\(\epsilon_i\)</span>-DP algorithms on the same data, the privacy budget becomes <span class="math notranslate nohighlight">\((\sum_{i=1}^k \epsilon_i)\)</span>-DP.
We can assign a budget to each user.</p>
</section>
</section>
</section>
<section id="utility-loss-in-interactive-databases">
<h3>Utility loss in interactive databases<a class="headerlink" href="#utility-loss-in-interactive-databases" title="Link to this heading">#</a></h3>
<p>For query restriction, utility loss can be measured as the number of refused queries.
For camouflage, utility loss is proportional to the width of the returned intervals.
For query perturbation, the difference between the true query response and the perturbed query response is a measure of utility loss ‚áí this can be characterized in terms of the mean and variance of the noise being added (ideally, the mean should be zero and the variance small).</p>
</section>
<section id="disclosure-risk-in-interactive-databases">
<h3>Disclosure risk in interactive databases<a class="headerlink" href="#disclosure-risk-in-interactive-databases" title="Link to this heading">#</a></h3>
<p>In query restriction, the query set size below which queries are refused is a measure of disclosure risk (a query set size 1 means total disclosure).
In camouflage, disclosure risk is inversely proportional to the interval width.
If query perturbation is used according to a privacy model like <span class="math notranslate nohighlight">\(\epsilon\)</span>-differential privacy, disclosure risk is controlled a priori by the <span class="math notranslate nohighlight">\(\epsilon\)</span> parameter (the lower, the less risk).</p>
</section>
</section>
<section id="microdata">
<h2>Microdata<a class="headerlink" href="#microdata" title="Link to this heading">#</a></h2>
<p>Microdata represents the most granular and detailed form of structured data: each record corresponds to an individual entity (a person, household, or organization), and each attribute describes some aspect of that entity‚Äîsuch as age, income, education, or health condition. Because microdata preserves individual-level information, it provides great analytical value for research and policymaking, but it also poses the highest privacy risks. Even after removing direct identifiers, individuals can often be re-identified by combining quasi-identifiers with external datasets. Microdata anonymization therefore seeks to transform such datasets to make re-identification or attribute inference practically impossible, while maintaining as much data utility as possible for statistical and analytical purposes.</p>
<p>Types of Attributes and Privacy Threats</p>
<p>As in previous discussions, attributes in microdata can be categorized into several classes:</p>
<p>Direct identifiers, such as name, ID number, or email, which uniquely identify an individual and must always be removed or masked.</p>
<p>Quasi-identifiers, which do not identify a person on their own but can do so in combination with other attributes (for example, age, ZIP code, and gender).</p>
<p>Confidential (or sensitive) attributes, which contain private information that should not be disclosed, such as salary, disease, or political affiliation.</p>
<p>Non-confidential attributes, which are neutral and generally harmless to share.</p>
<p>The main threats to privacy in microdata publication are re-identification (linking records to known individuals) and attribute disclosure (inferring sensitive information about an identified individual). These risks arise because even a few quasi-identifiers can uniquely characterize many people. A classic example is the discovery that 87% of Americans could be uniquely identified by the combination of ZIP code, gender, and date of birth.</p>
<p>Non-Perturbative vs. Perturbative Approaches</p>
<p>Anonymization methods for microdata are broadly divided into non-perturbative and perturbative techniques.</p>
<p>Non-perturbative methods preserve the truthfulness of data values but reduce their precision or granularity. This includes recoding, generalization, and suppression. For example, instead of publishing exact ages, data may be grouped into ranges (20‚Äì29, 30‚Äì39, etc.), or small geographic areas may be aggregated into larger regions. The idea is to ensure that each record becomes indistinguishable from several others with respect to the quasi-identifiers.</p>
<p>Perturbative methods, on the other hand, modify data values by introducing controlled randomness. This includes noise addition (slightly altering numerical values), data swapping (exchanging attribute values between records), rank swapping (reordering values among similar records), and synthetic data generation (creating entirely artificial records that preserve statistical distributions). These methods are particularly useful when precise micro-level accuracy is not required but overall statistical properties must remain valid.</p>
<p>Both categories aim to balance two competing objectives: minimizing disclosure risk and preserving analytical utility. The optimal choice depends on the purpose of the data release.</p>
<p>k-Anonymity: The Foundational Model</p>
<p>The most influential formal model for microdata anonymization is k-anonymity, introduced by Samarati and Sweeney in the late 1990s. A dataset satisfies k-anonymity if every combination of quasi-identifiers appears in at least k records. In other words, each individual is indistinguishable from at least k‚Äì1 others with respect to the identifying attributes. This means that the probability of correctly re-identifying any record is at most 1/k.</p>
<p>To achieve k-anonymity, the dataset is partitioned into equivalence classes, where each class contains at least k records that share the same generalized values for their quasi-identifiers. For example, if a dataset achieves 4-anonymity, each unique combination of age range, gender, and ZIP code must appear in at least four records. The process typically involves iterative generalization (grouping categorical values, widening numerical intervals) and suppression (removing overly unique records).</p>
<p>While k-anonymity provides a clear and intuitive privacy guarantee, it has limitations. It protects only against identity disclosure, not attribute disclosure. If all individuals in an equivalence class share the same value for a sensitive attribute‚Äîsay, they all have the same medical diagnosis‚Äîthen knowing that a person belongs to that group still reveals their confidential information. To address this, several stronger models were developed.</p>
<p>l-Diversity and t-Closeness: Beyond Identity Protection</p>
<p>l-Diversity extends k-anonymity by requiring that each equivalence class contain at least l well-represented values for every sensitive attribute. This ensures diversity within groups, reducing the risk of attribute disclosure. For instance, if an equivalence class of four records all indicate ‚ÄúHIV-positive,‚Äù the dataset is technically 4-anonymous but not diverse; an attacker would immediately learn that anyone in that group has HIV. Ensuring l-diversity means that sensitive attributes vary across the records in each group.</p>
<p>However, l-diversity can still fail when the overall distribution of a sensitive attribute is highly skewed. If one value (e.g., ‚Äúflu‚Äù) dominates globally, then even a diverse group might not protect well. To address this, t-closeness was introduced. It requires that the distribution of a sensitive attribute within each equivalence class be close (within a distance t) to its distribution in the entire dataset. The smaller t is, the stronger the privacy guarantee. This model directly measures how much information about sensitive attributes can be inferred from group membership, ensuring that each group reflects the overall population distribution.</p>
<p>These models form a hierarchy of protection:</p>
<p>k-anonymity prevents identity disclosure.</p>
<p>l-diversity reduces attribute disclosure by ensuring variety.</p>
<p>t-closeness controls how much the internal distribution can deviate from the global one.</p>
<p>Practical Techniques: Masking and Recoding</p>
<p>In practical applications, microdata anonymization is often achieved through masking techniques that modify quasi-identifiers according to chosen privacy parameters. Common strategies include:</p>
<p>Global recoding, where values are replaced with broader categories (e.g., merging job titles into sectors).</p>
<p>Local suppression, where certain values are removed or replaced with missing entries in specific records that threaten anonymity.</p>
<p>Top- and bottom-coding, which cap extreme numerical values‚Äîfor example, reporting all incomes above ‚Ç¨200,000 simply as ‚Äú‚â•200,000.‚Äù</p>
<p>Microaggregation, which replaces groups of similar records with group averages, commonly used for numerical data.</p>
<p>These transformations are carefully evaluated to ensure they maintain the statistical relationships necessary for analysis‚Äîsuch as means, variances, and correlations‚Äîwhile satisfying k-anonymity or its extensions.</p>
<p>Evaluating Risk and Utility</p>
<p>Every anonymization process involves measuring disclosure risk (the likelihood of re-identification) and information loss (the reduction in data quality or analytical power). Techniques such as risk‚Äìutility maps are used to visualize the trade-off between these two dimensions. Analysts can adjust parameters (e.g., the value of k or the granularity of generalization) to achieve the desired balance.</p>
<p>An additional consideration is the context of data release. For public data releases, stronger privacy (higher k or t) is required; for controlled research environments where access is restricted and monitored, weaker anonymization may suffice to retain utility.</p>
<p>Conceptual Transition: Toward Queryable and Differential Privacy</p>
<p>The anonymization of microdata bridges traditional statistical disclosure control and the more recent paradigm of differential privacy. While k-anonymity and its variants rely on modifying data before release, differential privacy introduces randomness during query processing. The former alters data values; the latter perturbs outputs. Both share the same goal‚Äîlimiting how much information about any individual can be inferred‚Äîbut differ in mathematical foundations and operational approach.</p>
<p>Microdata anonymization remains central in domains like healthcare, official statistics, and social research, where tabular or aggregate data is insufficient and researchers require individual-level information. It embodies the classic tension between privacy protection and scientific value, reminding us that the act of anonymization is not merely technical but also ethical: protecting individuals while preserving knowledge that can benefit society.</p>
<p>A microdata file <span class="math notranslate nohighlight">\(X\)</span> with <span class="math notranslate nohighlight">\(s\)</span> respondents and <span class="math notranslate nohighlight">\(t\)</span> attributes is an <span class="math notranslate nohighlight">\(s \times t\)</span> matrix where <span class="math notranslate nohighlight">\(X_{ij}\)</span> is the value of attribute <span class="math notranslate nohighlight">\(j\)</span> for respondent <span class="math notranslate nohighlight">\(i\)</span>.</p>
<p><img alt="alt text" src="_images/microdata_approaches.png" /></p>
<section id="id3">
<h3>Non-perturbative masking<a class="headerlink" href="#id3" title="Link to this heading">#</a></h3>
<section id="recoding">
<h4>Recoding<a class="headerlink" href="#recoding" title="Link to this heading">#</a></h4>
<p>Recoding is a non-perturbative, deterministic method for numeric and categorical attributes.
Recoding is used to decrease the number of distinct categories or values for a variable.
This is done by combining or grouping categories for categorical variables or constructing intervals for continuous variables.
Recoding is applied to all observations of a certain variable and not only to those at risk of disclosure.
There are two general types of recoding: global recoding and top and bottom coding.</p>
<section id="global-recoding">
<h5>Global recoding<a class="headerlink" href="#global-recoding" title="Link to this heading">#</a></h5>
<p>Global recoding combines several categories of a categorical variables or constructs intervals for continuous variables.</p>
<p><img alt="alt text" src="_images/global_recoding.png" /></p>
</section>
<section id="top-bottom-coding">
<h5>Top/bottom coding<a class="headerlink" href="#top-bottom-coding" title="Link to this heading">#</a></h5>
<p>Top and bottom coding are similar to global recoding, but instead of recoding all values, only the top and/or bottom values of the distribution or categories are recoded.</p>
<p><img alt="alt text" src="_images/top_coding.png" /></p>
</section>
<section id="rounding">
<h5>Rounding<a class="headerlink" href="#rounding" title="Link to this heading">#</a></h5>
<p>Rounding is similar to grouping but used for continuous variables.
Rounding is useful to prevent exact matching with external data sources.
In addition, it can be used to reduce the level of detail in the data.
Examples are removing decimal figures or rounding to the nearest 1,000.</p>
</section>
</section>
<section id="local-suppression">
<h4>Local suppression<a class="headerlink" href="#local-suppression" title="Link to this heading">#</a></h4>
<p>Certain values of individual attributes are suppressed to increase the set of records agreeing on a combination of quasi-identifier values.
Recoding reduces the number of necessary suppressions as well as the computation time needed for suppression.
Local suppression is not suitable for continuous variables or variables with a very high number of categories.
A possible solution in those cases might be to first recode to produce fewer categories.</p>
<p><img alt="alt text" src="_images/local_suppression.png" /></p>
<section id="recoding-suppression-and-k-anonymity">
<h5>Recoding, suppression, and ùëò-anonymity<a class="headerlink" href="#recoding-suppression-and-k-anonymity" title="Link to this heading">#</a></h5>
<p>The computational approach originally proposed by Samarati and Sweeney to achieve ùëò-anonymity combined recoding and suppression (the latter to reduce the need for the former).
Most of the ùëò-anonymity literature still relies on recoding, even though:
Recoding cannot preserve the numerical semantics of continuous attributes.
It uses a domain-level recoding hierarchy, rather than a data-driven one.</p>
</section>
</section>
<section id="pram">
<h4>PRAM<a class="headerlink" href="#pram" title="Link to this heading">#</a></h4>
<p>PRAM is a probabilistic perturbative method for categorical data.
Each value of a categorical attribute is changed to a different value according to a prescribed Markov matrix (PRAM matrix).
PRAM can afford transparency: publishing the PRAM matrix does not allow inverting anonymization.
PRAM is useful when a dataset contains many variables and applying other anonymization methods would lead to significant information loss.
A disadvantage of using PRAM is that very unlikely combinations can be generated.</p>
</section>
<section id="microaggregation">
<h4>Microaggregation<a class="headerlink" href="#microaggregation" title="Link to this heading">#</a></h4>
<p>The first step is the formation of small groups of individuals that are homogeneous with respect to the values of selected variables.
Then, the values of the selected variables of all group members are replaced with a common value.
The higher the within-group homogeneity, the lower the information loss.
Initially limited to continuous data, but it can also be applied to categorical data, using suitable definitions of distance and average.
With the introduction of word embeddings, such as word2vec, this is nowadays quite straightforward.</p>
</section>
<section id="noise-addition">
<h4>Noise addition<a class="headerlink" href="#noise-addition" title="Link to this heading">#</a></h4>
<p>Noise addition means adding or subtracting values to the original values of a variable and is most suited to protect continuous variables.
Noise addition can prevent exact matching of continuous variables.
The simplest version of noise addition is uncorrelated additive normally distributed noise <span class="math notranslate nohighlight">\(z_j=x_j+r_j\)</span>, where <span class="math notranslate nohighlight">\(r_j ~ N(0,  \sigma_{r_j}^2)\)</span> and <span class="math notranslate nohighlight">\(\sigma_{r_j} = \alpha \cdot \sigma_j\)</span>
If the level of noise added, <span class="math notranslate nohighlight">\(\alpha\)</span>, is disclosed to the user, many statistics can be consistently estimated from the perturbed data.
Option: calibrate noise using <span class="math notranslate nohighlight">\(\epsilon\)</span>-differential privacy.</p>
</section>
<section id="swapping">
<h4>Swapping<a class="headerlink" href="#swapping" title="Link to this heading">#</a></h4>
<p>Data swapping was presented for databases containing only categorical attributes.
Values of confidential attributes are exchanged among individual records, so that low-order frequency counts or marginals are maintained.
Rank swapping is a variant of data swapping, also applicable to numerical attributes.
Values of each attribute are ranked in ascending order and each value is swapped with another ranked value randomly chosen within a restricted range (e.g., the ranks of two swapped values cannot differ by more than ùëù% of the total number of records).</p>
</section>
</section>
<section id="synthetic-data-generation">
<h3>Synthetic data generation<a class="headerlink" href="#synthetic-data-generation" title="Link to this heading">#</a></h3>
<p>Idea: Randomly generate data in such a way that some statistics or relationships of the original data set are preserved.
Pros: No respondent re-identification seems possible, because data are synthetic.
Cons:
If a synthetic record matches by chance a respondent‚Äôs attributes, re-identification is likely and the respondent will find little comfort in the data being synthetic.
Data utility of synthetic microdata is limited to the statistics and relationships pre-selected at the outset.
Analyses on random subdomains are no longer preserved.
Partially synthetic or hybrid data are more flexible.</p>
</section>
<section id="utility-loss-in-microdata-protection">
<h3>Utility loss in microdata protection<a class="headerlink" href="#utility-loss-in-microdata-protection" title="Link to this heading">#</a></h3>
<p>Data utility:
Data use-specific utility loss measures.
Generic utility loss measures.
Disclosure risk:
Fixed a priori by a privacy model: <span class="math notranslate nohighlight">\(\epsilon\)</span>-differential privacy, <span class="math notranslate nohighlight">\(k\)</span>-anonymity.
Measured a posteriori by record linkage.</p>
<p>Number of suppressed records or values.
Number of modified records.
Means, variances, covariances, correlations‚Ä¶
Distance metrics between original and anonymized datasets.
Propensity score:
The propensity score is a measure of indistinguishability between original and anonymized datasets.
Merge the original and anonymized datasets and add a binary label <span class="math notranslate nohighlight">\(P\)</span> with value <span class="math notranslate nohighlight">\(1\)</span> for the anonymized records and <span class="math notranslate nohighlight">\(0\)</span> for the original records.
Predict <span class="math notranslate nohighlight">\(\hat{P}\)</span>, then
$<span class="math notranslate nohighlight">\(U = 1 - \frac{1}{4} \sum_{i=1}^N \left( \hat{p}_i - \frac{1}{2} \right)^2\)</span>$</p>
</section>
<section id="disclosure-risk-in-microdata-protection">
<h3>Disclosure risk in microdata protection<a class="headerlink" href="#disclosure-risk-in-microdata-protection" title="Link to this heading">#</a></h3>
<section id="a-priori-disclosure-risk">
<h4>A priori disclosure risk<a class="headerlink" href="#a-priori-disclosure-risk" title="Link to this heading">#</a></h4>
<p>Using a privacy model like <span class="math notranslate nohighlight">\(k\)</span>-anonymity or differential privacy allows the tolerable disclosure risk to be selected at the outset.
For <span class="math notranslate nohighlight">\(k\)</span>-anonymity the risk of identity disclosure is upper-bounded by <span class="math notranslate nohighlight">\(1/k\)</span>.
<span class="math notranslate nohighlight">\(\epsilon\)</span>-Differential privacy can ensure a very low identity and disclosure (esp. for small <span class="math notranslate nohighlight">\(\epsilon\)</span>), but at the expense of a great utility loss.</p>
<p>A data set is said to satisfy <span class="math notranslate nohighlight">\(k\)</span>-anonymity if each combination of values of the quasi-identifier attributes in it is shared by at least <span class="math notranslate nohighlight">\(k\)</span> records.
A data set is said to satisfy <span class="math notranslate nohighlight">\(\ell\)</span>-diversity if, for each group of records sharing a combination of key attributes, there are at least <span class="math notranslate nohighlight">\(\ell\)</span> ‚Äúwell-represented‚Äù values for each confidential attribute.
A data set is said to satisfy <span class="math notranslate nohighlight">\(t\)</span>-closeness if, for each group of records sharing a combination of quasi-identifiers, the distance between the distribution of the confidential attribute in the group and the distribution of the attribute in the whole data set is no more than a threshold <span class="math notranslate nohighlight">\(t\)</span>.</p>
</section>
<section id="a-posteriori-disclosure-risk">
<h4>A posteriori disclosure risk<a class="headerlink" href="#a-posteriori-disclosure-risk" title="Link to this heading">#</a></h4>
<p>The uniqueness approach
Typically used with non-perturbative masking.
It measures disclosure risk as the probability that rare combinations of attribute values in the released data are indeed rare in the original population the data come from.
The record linkage approach
Attempt to link records between the anonymized and original records, either with perfect matches or with some distance measures. % of correct guesses ‚Üí risk.
It can be applied to any type of masking and synthetic data.
It can even be applied to measure the actual disclosure risk of <span class="math notranslate nohighlight">\(\epsilon\)</span>-differentially private data releases.</p>
</section>
</section>
<section id="owner-privacy-privacy-preserving-data-mining">
<h3>Owner Privacy: Privacy-Preserving Data Mining<a class="headerlink" href="#owner-privacy-privacy-preserving-data-mining" title="Link to this heading">#</a></h3>
<p>Privacy-Preserving Data Mining (PPDM) tries to solve the following question: can
we develop accurate data mining models without access to the data at the record
level? Therefore, it consists of techniques for modifying the original data in such a
way that the private data remain private even after the mining process (Verykios
et al. 2004).
There are two radically different approaches to PPDM, namely, PPDM based on
perturbation and PPDM based on Secure Multiparty Computation (SMC). The first
was introduced by Agrawal and Srikant (2000) in the database community. Its idea
is that respondents (who do not wish to reveal the exact value of their respective
answers/records) or controllers (who wish to engage in joint computation with other
controllers without disclosing their respective data sets to each other) compute modified values for sensitive attributes in such a way that accurate statistical results can
still be obtained on the modified data. PPDM based on perturbation is largely based
on statistical disclosure control techniques.
PPDM based on SMC, which was introduced by Lindell and Pinkas (2000) in the
cryptographic community, addresses the problem of several entities holding confidential databases who wish to run a data mining algorithm on the union of their
databases, without revealing unnecessary information. This type of PPDM is equivalent to data mining in distributed environments, where the data are partitioned
across multiple parties. Partitioning can be vertical (each party holds all records on
a different subset of attributes), horizontal (each party holds a subset of the records,
but each record contains all attributes) or mixed.
Using SMC protocols based on cryptography (many of these resort to homomorphic encryption) or on sharing perturbed information in ways that do not alter the
final results often requires changing or adapting the data mining algorithms. Hence,
each cryptographic PPDM protocol is designed for a specific data mining computation and, in general, is not valid for other computations. For example, a secure scalar
product protocol based on cryptographic primitives is applied to privacy preserving
k-means clustering over a distributed dataset by Vaidya and Clifton (2003) and
Jagannathan and Wright (2005). Similarly, Du et al. (2004) and Karr et al. (2009) propose different ways (none of them based on encryption) to securely compute
matrix products, which permits obtaining privacy-preserving linear regressions.
A different PPDM scenario arises when a data controller wants to leverage the
storage and also the computational power of untrusted clouds to process her sensitive data. This setting was studied in the H2020 project ‚ÄòCLARUS‚Äô (<a class="reference external" href="http://clarussecure.eu">http://clarussecure.eu</a>) and solutions based on cleartext data splitting across several clouds have
been proposed. Furthermore, protocols to compute scalar products and matrix products with minimum controller involvement and maximum cloud involvement have
been given by Domingo-Ferrer et al. (2018).</p>
</section>
<section id="user-privacy-private-information-retrieval">
<h3>User Privacy: Private Information Retrieval<a class="headerlink" href="#user-privacy-private-information-retrieval" title="Link to this heading">#</a></h3>
<p>Finally, we address the privacy of the users querying a database. A history of queries
to a database, or to a web search engine, can be used by the database owner to learn
the interests of users, that is, to profile them. In this scenario, we seek to protect
users from unrequested profiling by database owners. Mechanisms to achieve this
goal are collectively referred to as private information retrieval (PIR).
Initial works on PIR, such as Chor et al. (1995), model databases as vectors of
entries. Users requesting information from the database do so by providing an
index or a set of indices of the database vector. In this setting, PIR techniques aim
to hide the indices provided by the users. However, these initial approaches have
several shortcomings. First, they require collaboration from the database owner,
something that cannot be ensured unless database owners have a clear incentive to
do so. Second, to perfectly hide the queried database indices one would need to
query all entries in the database and then filter the results locally, which is clearly
inefficient for moderately sized databases and certainly unfeasible for big databases. Finally, modelling a database as a vector and assuming that the user knows
the indices where the desired information is stored is not applicable to most real
databases, let alone web search engines.
Several solutions have been proposed to overcome such shortcomings. DomingoFerrer et al. (2009) propose a system named Goopir in which user queries are locally
complemented with terms of similar frequency in the language (connected by OR
operations). The responses are then filtered locally. TrackMeNot (Howe and
Nissenbaum 2009) is a browser extension which periodically sends fake queries to
web search engines so that the distribution of interests of the user is flattened and no
useful profile can be extracted. Finally, other proposals such as the one by Reiter
and Rubin (1998) make use of a P2P network in which users submit queries generated by other users to the web search engine, thus achieving the same results as
TrackMeNot (flattened interest distributions) but without overloading the web
search engines with fake queries.</p>
</section>
</section>
<div class="toctree-wrapper compound">
</div>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./."
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
              
              
                <footer class="prev-next-footer d-print-none">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="pets.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title">Privacy enhancing techniques</p>
      </div>
    </a>
    <a class="right-next"
       href="anonymization-library.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Anonymization library</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">


  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id1">Privacy in Databases</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#respondent-privacy-statistical-disclosure-control">Respondent Privacy: Statistical Disclosure Control</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#non-perturbative-masking">Non-perturbative Masking</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#perturbative-masking">Perturbative Masking</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#synthetic-microdata-generation">Synthetic Microdata Generation</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#privacy-models">Privacy Models</a><ul class="nav section-nav flex-column">
<li class="toc-h5 nav-item toc-entry"><a class="reference internal nav-link" href="#k-anonymity-and-extensions">k-Anonymity and Extensions</a></li>
<li class="toc-h5 nav-item toc-entry"><a class="reference internal nav-link" href="#differential-privacy">Differential Privacy</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#introduction">Introduction</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#tabular-data-protection">Tabular data protection</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#types-of-tables">Types of tables</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#disclosure-attacks-in-tables">Disclosure attacks in tables</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#sdc-methods-for-tables">SDC Methods for tables</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#cell-suppression">Cell suppression</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#sensitivity-rules">Sensitivity rules</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#secondary-suppressions">Secondary suppressions</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#controlled-rounding-and-controlled-tabular-adjustment">Controlled rounding and Controlled tabular adjustment</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#utility-loss-in-tabular-sdc">Utility loss in tabular SDC</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#disclosure-risk-in-tabular-sdc">Disclosure risk in tabular SDC</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#interactive-databases">Interactive databases</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#issues">Issues</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#interactive-database-protection">Interactive database protection</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#query-restriction">Query restriction</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#camouflage">Camouflage</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#query-perturbation">Query perturbation</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#randomized-response">Randomized response</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#id2">Differential privacy</a><ul class="nav section-nav flex-column">
<li class="toc-h5 nav-item toc-entry"><a class="reference internal nav-link" href="#the-laplace-mechanism">The Laplace mechanism</a></li>
<li class="toc-h5 nav-item toc-entry"><a class="reference internal nav-link" href="#properties-of-dp">Properties of DP</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#utility-loss-in-interactive-databases">Utility loss in interactive databases</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#disclosure-risk-in-interactive-databases">Disclosure risk in interactive databases</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#microdata">Microdata</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id3">Non-perturbative masking</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#recoding">Recoding</a><ul class="nav section-nav flex-column">
<li class="toc-h5 nav-item toc-entry"><a class="reference internal nav-link" href="#global-recoding">Global recoding</a></li>
<li class="toc-h5 nav-item toc-entry"><a class="reference internal nav-link" href="#top-bottom-coding">Top/bottom coding</a></li>
<li class="toc-h5 nav-item toc-entry"><a class="reference internal nav-link" href="#rounding">Rounding</a></li>
</ul>
</li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#local-suppression">Local suppression</a><ul class="nav section-nav flex-column">
<li class="toc-h5 nav-item toc-entry"><a class="reference internal nav-link" href="#recoding-suppression-and-k-anonymity">Recoding, suppression, and ùëò-anonymity</a></li>
</ul>
</li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#pram">PRAM</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#microaggregation">Microaggregation</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#noise-addition">Noise addition</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#swapping">Swapping</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#synthetic-data-generation">Synthetic data generation</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#utility-loss-in-microdata-protection">Utility loss in microdata protection</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#disclosure-risk-in-microdata-protection">Disclosure risk in microdata protection</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#a-priori-disclosure-risk">A priori disclosure risk</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#a-posteriori-disclosure-risk">A posteriori disclosure risk</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#owner-privacy-privacy-preserving-data-mining">Owner Privacy: Privacy-Preserving Data Mining</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#user-privacy-private-information-retrieval">User Privacy: Private Information Retrieval</a></li>
</ul>
</li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Alberto Blanco-Justicia
</p>

  </div>
  
  <div class="footer-item">
    

  <p class="copyright">
    
      ¬© Copyright 2023.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b"></script>
<script src="_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>