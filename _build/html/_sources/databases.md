# Privacy in databases

## Introduction

In healthcare, data is often collected from patients to better understand disease patterns, improve treatments, and develop new medications. 
Some common types of data that are often used in biomedical research include: 
Demographic data: age, address, marital status, gender, etc.
Social data: education, income, etc. 
Behavioral data: sleeping habits, smoker, drinker, drug user, etc.
Clinical data: diagnostics, medications, test results, etc. 
Genomic data: genomic sequencing from samples.
Image data: x-rays, CT-scans, MRI, fMRI, etc.
Textual data: extra auxiliary information.

When processing, sharing, or releasing these data, we might do so as:
Tabular data. Tables with counts or magnitudes.
Queryable databases. On-line databases which accept statistical queries (sums, averages, max, min, etc.).
Microdata. Files where each record contains information on an individual (a physical person or an organization).

Statistical databases must provide useful statistical information.
They must also preserve the privacy of respondents if data are sensitive.
Anonymization measures aim to produce modified versions of the original datasets which are as close as possible to the original datasets but do not allow attackers to identify individuals or to infer confidential attributes about specific individuals. 
Anonymous data are not classified as personal data and hence fall outside the scope of the GDPR. 

Whichever the type and format of the data, the general aim of anonymization mechanisms is to minimize the risk of disclosure, in particular identity and/or attribute disclosure. 
In addition to accidental leakages, such disclosures can occur because of attacks. 
Record and attribute linkage attacks match auxiliary knowledge of an individual to records in private microdata sets to learn additional information about the target.
Membership and attribute inference leverage released aggregate data, such as statistical tables or machine learning models, to improve their probabilistic belief in some piece of information about a target individual.
Finally, reconstruction attacks try to reconstruct the original microdata (or training data in case of ML) from released statistics (or ML models). 

Evaluation is in terms of two conflicting goals:
Minimize the data utility loss caused by the method.
Minimize the extant disclosure risk in the anonymized data.
The best methods are those that optimize the trade-off between both goals.
Two approaches exist to achieve this trade-off:
Utility-first anonymization: apply masking methods iteratively until a desired disclosure risk is met.
Privacy-first anonymization: use a privacy model, such as ùëò-anonymity or ùúñ-differential privacy, which sets a disclosure risk, to derive parameters for masking methods.



## Tabular data protection

Goal: Publish static aggregate information, in such a way that no confidential information can be inferred on specific individuals to whom the table refers.
From microdata, tabular data can be generated by crossing one or more categorical attributes.
Formally, given categorical attributes $X_1, \cdots, X_l$, a table $T$ is a function
$$T : D(X_1) \times D(X_2) \times \cdots \times D(X_l) \rightarrow R \; or \; N$$
where $D(X_i)$ is the domain where attribute $X_i$ takes its values.


### Types of tables

| id | Salary  | Sector  | Region  |
|--- |---      |---      |---      |
|  1 | 500000‚Ç¨  | IT      | BCN  |
|  2 | 320000 ‚Ç¨ | IT      | BCN  |
|  3 | 32000 ‚Ç¨  | IT      | BCN  |
|  4 | 35000 ‚Ç¨  | IT      | TGN  |
|  5 | 34000 ‚Ç¨  | IT      | TGN  |
|  6 | 28000 ‚Ç¨  | IT      | TGN  |
|  7 | 300000 ‚Ç¨ | HC      | BCN  |
|  8 | 45000 ‚Ç¨  | HC      | BCN  |
|  9 | 34000 ‚Ç¨  | HC      | BCN  |
| 10 | 45000 ‚Ç¨  | HC      | TGN  |
| 11 | 34000 ‚Ç¨  | HC      | TGN  |
| 12 | 24000 ‚Ç¨  | HC      | TGN  |
| 13 | 300000 ‚Ç¨ | Fin     | BCN  |
| 14 | 350000 ‚Ç¨ | Fin     | BCN  |
| 15 | 150000 ‚Ç¨ | Fin     | TGN  |

<table>
    <tr>
        <th> Frequency </th>
        <th style="text-align: center" colspan=3> Region </th>
    </tr>
    <tr>
        <th> <b>Sector</b> </th>
        <th> BCN </th>
        <th> TGN </th>
        <th> Total </th>
    </tr>
        <td> IT </td>
        <td> 3 </td>
        <td> 3 </td>
        <td> 6 </td>
    <tr>
        <td> Fin </td>
        <td> 2 </td>
        <td> 1 </td>
        <td> 3 </td>
    </tr>
    <tr>
        <td> HC </td>
        <td> 3 </td>
        <td> 3 </td>
        <td> 6 </td>
    </tr>
    <tr>
        <td> Total </td>
        <td> 8 </td>
        <td> 7 </td>
        <td> 15 </td>
    </tr>
</table>

<table>
    <tr>
        <th> Sum of Salary </th>
        <th style="text-align: center" colspan=3> Region </th>
    </tr>
    <tr>
        <th> <b>Sector</b> </th>
        <th> BCN </th>
        <th> TGN </th>
        <th> Total </th>
    </tr>
        <td> IT </td>
        <td> 852000 ‚Ç¨ </td>
        <td> 97000 ‚Ç¨ </td>
        <td> 949000 ‚Ç¨ </td>
    <tr>
        <td> Fin </td>
        <td> 335000 ‚Ç¨ </td>
        <td> 150000 ‚Ç¨ </td>
        <td> 485000 ‚Ç¨ </td>
    </tr>
    <tr>
        <td> HC </td>
        <td> 379000 ‚Ç¨ </td>
        <td> 103000 ‚Ç¨ </td>
        <td> 482000 ‚Ç¨ </td>
    </tr>
    <tr>
        <td> Total </td>
        <td> 1566000 ‚Ç¨ </td>
        <td> 350000 ‚Ç¨ </td>
        <td> 1916000 ‚Ç¨ </td>
    </tr>
</table>


### Disclosure attacks in tables

External attack 
Let a released frequency table **Ethnicity** $\times$ **Town** contain a single respondent for ethnicity $E_i$ and town $T_j$.
If a magnitude table is released with the average blood pressure for each ethnicity and each town, the exact blood pressure of the only respondent with ethnicity $ùê∏_ùëñ$ in town $ùëá_ùëó$ is publicly disclosed.
Internal attack 
If there are only two respondents for ethnicity $ùê∏_ùëñ$ and town $ùëá_ùëó$ , the blood pressure of each of them is disclosed to the other.
Dominance attack 
If one (or few) respondents dominate in the contribution to a cell in a magnitude table, the dominant respondent(s) can upper-bound the contributions of the rest. 
*E.g.*, if the table displays the cumulative earnings for each job type and town, and one individual contributes 90% of a certain cell value, they know their colleagues in the town are not doing very well.


### SDC Methods for tables

Non-perturbative. They do not modify the values in the cells, but they may suppress or recode them. 
Best known methods: 
cell suppression (CS).
recoding of categorical attributes.
Perturbative. They modify the values in the cells. 
Best known methods: 
controlled rounding (CR).
controlled tabular  adjustment (CTA).

#### Cell suppression

Identify sensitive cells, using a sensitivity rule.
Suppress values in sensitive cells (primary suppressions).
Perform additional suppressions (secondary suppressions) to prevent recovery of primary suppressions from row and/or column marginals.


#### Sensitivity rules

Minimum frequency rule: the cell frequency is less than a pre-specified minimum frequency ùëõ.
(ùíè,ùíå)-dominance rule: the sum of the ùëõ largest contributors exceeds ùëò% of the cell total.
ùíë%-rule: the cell total minus the 2 biggest contributors is less than ùëù% of the largest contribution.
ùíëùíí-rule: Generalization of the ùëù%-rule, which takes into account prior information. The 2nd biggest contributor can estimate the rest of contributions within ùëû%.

<table>
    <tr>
        <th> Sum of Salary </th>
        <th style="text-align: center" colspan=3> Region </th>
    </tr>
    <tr>
        <th> <b>Sector</b> </th>
        <th> BCN </th>
        <th> TGN </th>
        <th> Total </th>
    </tr>
        <td> IT </td>
        <td style="color:#FF0000"> 852000 ‚Ç¨ </td>
        <td> 97000 ‚Ç¨ </td>
        <td> 949000 ‚Ç¨ </td>
    <tr>
        <td> Fin </td>
        <td style="color:#FF0000"> 335000 ‚Ç¨ </td>
        <td style="color:#FF0000"> 150000 ‚Ç¨ </td>
        <td> 485000 ‚Ç¨ </td>
    </tr>
    <tr>
        <td> HC </td>
        <td> 379000 ‚Ç¨ </td>
        <td> 103000 ‚Ç¨ </td>
        <td> 482000 ‚Ç¨ </td>
    </tr>
    <tr>
        <td> Total </td>
        <td> 1566000 ‚Ç¨ </td>
        <td> 350000 ‚Ç¨ </td>
        <td> 1916000 ‚Ç¨ </td>
    </tr>
</table>


#### Secondary suppressions

Usually, one attempts to minimize either the number of secondary suppressions or their marginals.
Optimization methods are heuristic, based on mixed linear integer programming or networks flows (the latter for 2-D tables only).

<table>
    <tr>
        <th> Sum of Salary </th>
        <th style="text-align: center" colspan=3> Region </th>
    </tr>
    <tr>
        <th> <b>Sector</b> </th>
        <th> BCN </th>
        <th> TGN </th>
        <th> Total </th>
    </tr>
        <td> IT </td>
        <td> - </td>
        <td> 97000 ‚Ç¨ </td>
        <td> 949000 ‚Ç¨ </td>
    <tr>
        <td> Fin </td>
        <td> - </td>
        <td> - </td>
        <td> 485000 ‚Ç¨ </td>
    </tr>
    <tr>
        <td> HC </td>
        <td> 379000 ‚Ç¨ </td>
        <td> 103000 ‚Ç¨ </td>
        <td> 482000 ‚Ç¨ </td>
    </tr>
    <tr>
        <td> Total </td>
        <td> 1566000 ‚Ç¨ </td>
        <td> 350000 ‚Ç¨ </td>
        <td> 1916000 ‚Ç¨ </td>
    </tr>
</table>

A Feasibility Interval is the range of possible values that a suppressed cell can take, given the linear relations between cells and the row and column totals. 
In here, $X_{11}$ must be in range $[3,6]$, because
$ùëã_{21}+ùëã_{22}= 3 \rightarrow ùëã_{21} \le 3$,
$ùëã_{11}+ùëã_{21}+3=9 \rightarrow ùëã_{11} \le 6$, and 
$ùëã_{11}+ùëã_{21}+3=9  \rightarrow ùëã_{11} \ge 3$.


<table>
    <tr>
        <th> Example </th>
        <th style="text-align: center" colspan=3> Attribute 1 </th>
    </tr>
    <tr>
        <th> <b>Attribute 2</b> </th>
        <th> 1 </th>
        <th> 2 </th>
        <th> Total </th>
    </tr>
        <td> 1 </td>
        <td> X11 </td>
        <td> X12 </td>
        <td> 7</td>
    <tr>
        <td> 2 </td>
        <td> X21 </td>
        <td> X22 </td>
        <td> 3 </td>
    </tr>
    <tr>
        <td> 3 </td>
        <td> 3 </td>
        <td> 3 </td>
        <td> 6 </td>
    </tr>
    <tr>
        <td> Total </td>
        <td> 9 </td>
        <td> 7 </td>
        <td> 16 </td>
    </tr>
</table>

The feasibility interval must contain the protection interval. 
The protection interval is computed according to the chosen sensitivity rule $[ùê∂ùëâ‚àíùëÉùêø, ùê∂ùëâ+ùëÉùêø]$.

![alt text](figs/protection_levels.png)

#### Controlled rounding and Controlled tabular adjustment

CR rounds values in the table to multiples of a rounding base (marginals may have to be rounded as well).
CTA modifies the values in the table to prevent inference of sensitive cell values within a prescribed protection interval.
CTA attempts to find the closest table to the original one that protects all sensitive cells.
CTA optimization is typically based on mixed linear integer programming and entails less information loss than CS.


### Utility loss in tabular SDC

For cell suppression, utility loss is measured as the number of secondary suppressions or their pooled magnitude.
For controlled tabular adjustment or rounding, it is measured as the sum of distances between true and perturbed cell values.
The above loss measures may be weighted by cell costs, if not all cells have the same importance.


### Disclosure risk in tabular SDC

Disclosure risk is evaluated by computing the feasibility intervals for sensitive cells (via linear programming constrained by the marginals).
What could be the range of values of the suppressed cells given other cells, including marginals?
The table is safe if the feasibility interval for any sensitive cell contains the protection interval defined for that cell.
If a cell was suppressed because of the ùëù%-rule, does this ùëù% lie within the feasibility interval?
The protection interval is the narrowest interval estimate of the sensitive cell permitted by the data protector.


## Interactive databases

![alt text](figs/interactive_db.png)

### Issues

| id | Salary  | Sector  | Region  |
|--- |---      |---      |---      |
|  1 | 500000‚Ç¨  | IT      | BCN  |
|  2 | 320000 ‚Ç¨ | IT      | BCN  |
|  3 | 32000 ‚Ç¨  | IT      | BCN  |
|  4 | 35000 ‚Ç¨  | IT      | TGN  |
|  5 | 34000 ‚Ç¨  | IT      | TGN  |
|  6 | 28000 ‚Ç¨  | IT      | TGN  |
|  7 | 300000 ‚Ç¨ | HC      | BCN  |
|  8 | 45000 ‚Ç¨  | HC      | BCN  |
|  9 | 34000 ‚Ç¨  | HC      | BCN  |
| 10 | 45000 ‚Ç¨  | HC      | TGN  |
| 11 | 34000 ‚Ç¨  | HC      | TGN  |
| 12 | 24000 ‚Ç¨  | HC      | TGN  |
| 13 | 300000 ‚Ç¨ | Fin     | BCN  |
| 14 | 350000 ‚Ç¨ | Fin     | BCN  |
| 15 | 150000 ‚Ç¨ | Fin     | TGN  |

```sql
SELECT count() FROM table WHERE Salary > 100.000
Result = 5
SELECT sum(Salary) FROM table WHERE Sector = HC
Result = 482.000
SELECT count() FROM table WHERE Sector = IT and Region = TGN
Result = 3
SELECT count() FROM table WHERE Sector = Fin and Region = TGN
Result = 1
SELECT avg(Salary) FROM table WHERE Sector = Fin and Region = TGN
Result = 150.000
```


### Interactive database protection

Three main SDC approaches:
Query restriction. The database refuses to answer certain queries.
Camouflage. Deterministically correct non-exact answers (small interval answers) are returned by the database.
Query perturbation. Perturbation (noise addition) can be applied to the microdata records on which queries are computed (input perturbation) or to the query result after computing it on the original data (output perturbation).


#### Query restriction

This is the right approach if the user does require deterministically correct answers and these answers must be exact (i.e., a number).
Exact answers may be very disclosive, so it may be necessary to refuse answering certain queries at some stage.
A common criterion to decide whether a query can be answered is query set size control: the answer to a query is refused if this query together with the previously answered ones isolates too small a set of records.
Problems: computational burden to keep track of previous queries, collusion possible.


#### Camouflage

Interval answers are returned rather than point answers.
No distortion of answers.
Unlimited answers can be returned.
Each query can only concern a single confidential attribute.
Privacy bounds are set for each confidential attribute.
The data curator can set a global bound, e.g., 30% (ùë¢_ùëñ‚àí‚Ñì_ùëñ‚â•0.3ùë•_ùëñ).
Respondents may request specific upper and lower bounds for their values.


#### Query perturbation

Add noise to the values of confidential attributes in the original microdata (input perturbation). 
We will cover that when we talk about microdata protection.
Add noise to the results of the queries after computing them on the original data (output perturbation).
Under the differential privacy model, we can give theoretical guarantees on the privacy level achieved with output perturbation.


#### Randomized response

There are $n$ people, and individual $i$ has a sensitive bit $X_i \in \{0,1\}$.
Each person sends the analyst a message $Y_i$, which may depend on $X_i$ and some random numbers which the individual can generate.
Based on these $Y_i$, the analyst would like to estimate $p=1/n \sum_i X_i$. 
Consider the following strategy, which we will call Randomized Response, parameterized by some $\gamma\in [0, 1/2]$.
$Y_i = X_i$ with probability $1/2 + \gamma$ and $Y_i = 1-X_i$ with probability $1/2-\gamma$.
How private $Y_i$ and how accurate will the estimation of $y$? It depends on $\gamma$. There is plausible deniability.

#### Differential privacy

We imagine there are $n$ individuals, $X_1$ through $X_n$, who each have their own datapoint.
They send this point to a "trusted curator".
Given their data, the curator runs an algorithm $M$, and publicly outputs the result of this computation. 
Differential privacy is a property of $M$ saying that no individual's data has a large impact on the output.
Suppose we have an algorithm $M:X^n \rightarrow Y$. Consider any two datasets $X, X' \in X^n$ which differ in exactly one entry. We call these neighboring datasets. We say that $M$ is $\epsilon$-differentially private ($\epsilon$-DP) if, for all neighboring $X, X'$, and all $T \subseteq Y$, we have
$$Pr‚Å°[M(X) \in T] \le e^\epsilon Pr‚Å°[M(X') \in T]$$
where the randomness is over the choices made by $M$.

The sensitivity of a function $\Delta f$ (or $\Delta$ if $f$ is known) is the maximum distance between the results of $f$ computed on neighboring datasets.
This can be defined as the $\ell_1$ distance between results of $M$ on neighboring datasets ($(\epsilon,\delta)$-DP requires the $\ell_2$ distance).
For example:
- Count queries have a $\Delta$ of 1.
- Average queries have a $\Delta$ of $\max/n$.
- Sum queries have a $\Delta$ of $\max$.

##### The Laplace mechanism

The Laplace mechanism is a fundamental technique for achieving differential privacy. 
Given a function $f: D \rightarrow R^d, where $D$ is the domain of the dataset and $d$ is the dimension of the output, the Laplace mechanism $M_{Laplace}$ adds Laplace noise to the output of $f$. That is $M_{Laplace}(D)=f(D)+Lap(\Delta/\epsilon)$ 
Let $b$ be the scale parameter of the Laplace distribution, which is given by:
$$Lap(b)=\frac{1}{2b} e^{-\frac{|x|}{b}}$$

![alt text](figs/laplace.png)

##### Properties of DP

$\epsilon$ (privacy budget) should be small. Ideally in the range $(0,1]$.
Resistance to post-processing. Once a quantity is privatized, it can't be "un-privatized," if the data is not used again.
Group differential privacy. When neighboring datasets differ in $k$ positions, the guarantee goes to $k\epsilon-DP$.
Composition rules. If we run $k$ $\epsilon_i$-DP algorithms on the same data, the privacy budget becomes $(\sum_{i=1}^k \epsilon_i)$-DP.
We can assign a budget to each user.


### Utility loss in interactive databases

For query restriction, utility loss can be measured as the number of refused queries.
For camouflage, utility loss is proportional to the width of the returned intervals.
For query perturbation, the difference between the true query response and the perturbed query response is a measure of utility loss ‚áí this can be characterized in terms of the mean and variance of the noise being added (ideally, the mean should be zero and the variance small).


### Disclosure risk in interactive databases

In query restriction, the query set size below which queries are refused is a measure of disclosure risk (a query set size 1 means total disclosure).
In camouflage, disclosure risk is inversely proportional to the interval width.
If query perturbation is used according to a privacy model like $\epsilon$-differential privacy, disclosure risk is controlled a priori by the $\epsilon$ parameter (the lower, the less risk).


## Microdata

A microdata file $X$ with $s$ respondents and $t$ attributes is an $s \times t$ matrix where $X_{ij}$ is the value of attribute $j$ for respondent $i$.

![alt text](figs/microdata_approaches.png)


### Non-perturbative masking

#### Recoding

Recoding is a non-perturbative, deterministic method for numeric and categorical attributes.
Recoding is used to decrease the number of distinct categories or values for a variable.
This is done by combining or grouping categories for categorical variables or constructing intervals for continuous variables.
Recoding is applied to all observations of a certain variable and not only to those at risk of disclosure.
There are two general types of recoding: global recoding and top and bottom coding.

##### Global recoding

Global recoding combines several categories of a categorical variables or constructs intervals for continuous variables.

![alt text](figs/global_recoding.png)

##### Top/bottom coding

Top and bottom coding are similar to global recoding, but instead of recoding all values, only the top and/or bottom values of the distribution or categories are recoded. 

![alt text](figs/top_coding.png)

##### Rounding

Rounding is similar to grouping but used for continuous variables. 
Rounding is useful to prevent exact matching with external data sources. 
In addition, it can be used to reduce the level of detail in the data. 
Examples are removing decimal figures or rounding to the nearest 1,000.


#### Local suppression

Certain values of individual attributes are suppressed to increase the set of records agreeing on a combination of quasi-identifier values.
Recoding reduces the number of necessary suppressions as well as the computation time needed for suppression.
Local suppression is not suitable for continuous variables or variables with a very high number of categories. 
A possible solution in those cases might be to first recode to produce fewer categories.

![alt text](figs/local_suppression.png)

##### Recoding, suppression, and ùëò-anonymity

The computational approach originally proposed by Samarati and Sweeney to achieve ùëò-anonymity combined recoding and suppression (the latter to reduce the need for the former).
Most of the ùëò-anonymity literature still relies on recoding, even though:
Recoding cannot preserve the numerical semantics of continuous attributes.
It uses a domain-level recoding hierarchy, rather than a data-driven one.

#### PRAM

PRAM is a probabilistic perturbative method for categorical data.
Each value of a categorical attribute is changed to a different value according to a prescribed Markov matrix (PRAM matrix).
PRAM can afford transparency: publishing the PRAM matrix does not allow inverting anonymization.
PRAM is useful when a dataset contains many variables and applying other anonymization methods would lead to significant information loss.
A disadvantage of using PRAM is that very unlikely combinations can be generated.


#### Microaggregation

The first step is the formation of small groups of individuals that are homogeneous with respect to the values of selected variables. 
Then, the values of the selected variables of all group members are replaced with a common value. 
The higher the within-group homogeneity, the lower the information loss.
Initially limited to continuous data, but it can also be applied to categorical data, using suitable definitions of distance and average.
With the introduction of word embeddings, such as word2vec, this is nowadays quite straightforward.


#### Noise addition

Noise addition means adding or subtracting values to the original values of a variable and is most suited to protect continuous variables.
Noise addition can prevent exact matching of continuous variables. 
The simplest version of noise addition is uncorrelated additive normally distributed noise $z_j=x_j+r_j$, where $r_j ~ N(0,  \sigma_{r_j}^2)$ and $\sigma_{r_j} = \alpha \cdot \sigma_j$
If the level of noise added, $\alpha$, is disclosed to the user, many statistics can be consistently estimated from the perturbed data. 
Option: calibrate noise using $\epsilon$-differential privacy.


#### Swapping

Data swapping was presented for databases containing only categorical attributes.
Values of confidential attributes are exchanged among individual records, so that low-order frequency counts or marginals are maintained.
Rank swapping is a variant of data swapping, also applicable to numerical attributes.
Values of each attribute are ranked in ascending order and each value is swapped with another ranked value randomly chosen within a restricted range (e.g., the ranks of two swapped values cannot differ by more than ùëù% of the total number of records).


### Synthetic data generation

Idea: Randomly generate data in such a way that some statistics or relationships of the original data set are preserved.
Pros: No respondent re-identification seems possible, because data are synthetic.
Cons:
If a synthetic record matches by chance a respondent‚Äôs attributes, re-identification is likely and the respondent will find little comfort in the data being synthetic.
Data utility of synthetic microdata is limited to the statistics and relationships pre-selected at the outset.
Analyses on random subdomains are no longer preserved.
Partially synthetic or hybrid data are more flexible.


### Utility loss in microdata protection

Data utility:
Data use-specific utility loss measures.
Generic utility loss measures.
Disclosure risk:
Fixed a priori by a privacy model: $\epsilon$-differential privacy, $k$-anonymity.
Measured a posteriori by record linkage.

Number of suppressed records or values.
Number of modified records.
Means, variances, covariances, correlations‚Ä¶
Distance metrics between original and anonymized datasets.
Propensity score:
The propensity score is a measure of indistinguishability between original and anonymized datasets. 
Merge the original and anonymized datasets and add a binary label $P$ with value $1$ for the anonymized records and $0$ for the original records.
Predict $\hat{P}$, then
$$U = 1 - \frac{1}{4} \sum_{i=1}^N \left( \hat{p}_i - \frac{1}{2} \right)^2$$


### Disclosure risk in microdata protection

#### A priori disclosure risk 
Using a privacy model like $k$-anonymity or differential privacy allows the tolerable disclosure risk to be selected at the outset.
For $k$-anonymity the risk of identity disclosure is upper-bounded by $1/k$.
$\epsilon$-Differential privacy can ensure a very low identity and disclosure (esp. for small $\epsilon$), but at the expense of a great utility loss.

A data set is said to satisfy $k$-anonymity if each combination of values of the quasi-identifier attributes in it is shared by at least $k$ records.
A data set is said to satisfy $\ell$-diversity if, for each group of records sharing a combination of key attributes, there are at least $\ell$ "well-represented" values for each confidential attribute.
A data set is said to satisfy $t$-closeness if, for each group of records sharing a combination of quasi-identifiers, the distance between the distribution of the confidential attribute in the group and the distribution of the attribute in the whole data set is no more than a threshold $t$.

#### A posteriori disclosure risk
The uniqueness approach
Typically used with non-perturbative masking.
It measures disclosure risk as the probability that rare combinations of attribute values in the released data are indeed rare in the original population the data come from.
The record linkage approach
Attempt to link records between the anonymized and original records, either with perfect matches or with some distance measures. % of correct guesses ‚Üí risk. 
It can be applied to any type of masking and synthetic data.
It can even be applied to measure the actual disclosure risk of $\epsilon$-differentially private data releases.


## FLUFF

### Tabular

Anonymization of structured databases is one of the central technical methods for protecting personal data while preserving its analytical or statistical value. Structured data, such as that stored in relational databases or spreadsheets, consists of well-defined attributes arranged in rows and columns‚Äîeach row representing an individual and each column representing an attribute (such as age, income, or medical condition). These datasets are the foundation of much statistical, social, and biomedical research, yet they also pose serious privacy risks because individuals can often be identified or their attributes inferred, even after direct identifiers have been removed. The purpose of anonymization is to transform such data so that it can be used, published, or shared with third parties without revealing or allowing the inference of personal information.

To understand how anonymization works, it is essential to distinguish between different types of data attributes. Direct identifiers are attributes that uniquely identify a person on their own‚Äîsuch as a name, personal identification number, email address, or phone number. These are typically removed entirely in any anonymization process. Quasi-identifiers (or indirect identifiers) are attributes that do not identify an individual by themselves but can do so when combined with other information. Common examples include combinations of birth date, postal code, and gender, which have been shown to uniquely identify large portions of a population when cross-referenced with external data sources. Confidential attributes are those that are sensitive in nature, such as salary, medical diagnosis, or political opinion; these are not used to identify individuals but must be protected against disclosure. Finally, non-confidential attributes are those that carry little privacy risk and can typically be retained as-is.

When a dataset is released‚Äîeither publicly or to specific researchers‚Äîthe main risks are re-identification (linking data to an individual) and attribute disclosure (inferring confidential information about an identified person). These threats may arise from linkage attacks, in which an attacker combines anonymized data with external datasets, such as public voter registries or social media profiles, to re-establish identities. The well-known case of Latanya Sweeney‚Äôs re-identification of Massachusetts health records using public voter data illustrates this danger: even after names were removed, the combination of ZIP code, birth date, and sex uniquely identified a large percentage of individuals.

Structured data can appear in various forms depending on how it is shared. The simplest is tabular data, where aggregated statistics (like counts or averages) are published instead of individual records. While this reduces the risk of exposure, even aggregates can sometimes leak information when groups are small. Microdata releases contain individual-level records and pose higher risks but are more valuable for analysis. A third model involves queryable databases, where external users can run limited statistical queries without direct access to the data‚Äîan approach discussed more deeply in later sessions.

The core challenge of anonymization lies in finding the right balance between utility and privacy. If data is heavily modified or generalized, it may lose its usefulness for research or policy analysis; if it is insufficiently transformed, it remains vulnerable to privacy breaches. This balance can be described through two key measures: disclosure risk, the probability that an individual can be re-identified or a sensitive attribute inferred; and utility loss, the degree to which the anonymized data deviates from the original in ways that reduce its analytical value. In practice, anonymization often involves iterative processes that evaluate and adjust this trade-off.

Various approaches exist to anonymize structured data. A broad distinction can be made between non-perturbative and perturbative methods. Non-perturbative methods modify the structure of the data‚Äîby removing or grouping records, generalizing attribute values, or suppressing rare combinations‚Äîwithout changing the actual data values themselves. Examples include recoding numerical values into intervals (e.g., replacing exact ages with age ranges) or aggregating geographic details (replacing ‚Äústreet address‚Äù with ‚Äúpostal district‚Äù). Perturbative methods, on the other hand, introduce controlled randomness or noise to mask individual contributions while preserving global patterns. These include techniques like data swapping (exchanging values between records), noise addition (slightly altering numeric values), or synthetic data generation (creating artificial records that preserve statistical distributions).

To formalize privacy guarantees, researchers and practitioners use different models. The most widely known is k-anonymity, which requires that each record in a dataset be indistinguishable from at least k‚Äì1 others with respect to the quasi-identifiers. In other words, any given combination of identifying attributes must appear in at least k rows, making the probability of linking a record to a specific individual at most 1/k. Variants of this model, such as l-diversity and t-closeness, strengthen protection by ensuring diversity and similarity constraints within groups, reducing the risk of attribute disclosure even when sensitive attributes are unevenly distributed.

While these formal models are primarily applied to microdata, their principles also guide the anonymization of other data forms, including tabular and queryable data. In tabular data, cell suppression and controlled rounding are used to prevent inferences about small subgroups, while complementary suppression ensures that suppressed cells cannot be reconstructed from totals. When publishing frequency tables or census data, for instance, agencies routinely withhold or adjust values in small population groups to prevent re-identification.

An effective anonymization process typically begins with a detailed risk assessment. This includes identifying potential quasi-identifiers, estimating disclosure risk based on external data availability, and selecting appropriate transformation methods. After anonymization, utility evaluation ensures that the transformed data still supports the intended analyses‚Äîsuch as statistical inference, machine learning, or policy evaluation. Because anonymization is context-dependent, the same dataset might require different transformations depending on its intended use and audience: a public release requires stronger privacy, while restricted research access may allow more detail.

Ultimately, anonymization of structured databases illustrates a core theme in privacy engineering: privacy is not achieved by deleting data but by transforming it intelligently. It is a balance between protecting individuals and preserving collective knowledge. In practice, achieving full anonymity is nearly impossible‚Äîespecially as external data sources multiply‚Äîbut well-designed anonymization greatly reduces risk while maintaining social and scientific value.

Tabular data is one of the most common ways in which statistical information is published. Government agencies, research institutions, and companies often release aggregated tables summarizing results from surveys, censuses, or databases‚Äîfor example, the number of people by region and profession, or the average salary by sector. Although such tables appear harmless because they contain no individual records, they can still reveal sensitive information about individuals when analyzed carefully. Protecting privacy in tabular data publishing therefore requires specialized anonymization techniques that prevent both identity disclosure and attribute disclosure while maintaining the utility of the statistics.

A statistical table is usually constructed from microdata, meaning a collection of individual records that include multiple attributes for each person. When a table is built, those records are grouped according to certain key variables or quasi-identifiers (for example, region, age group, or gender), and a summary statistic‚Äîsuch as a count, total income, or average‚Äîis computed for each combination of categories. The resulting table thus contains cells, each representing a subgroup of individuals. For example, one cell might correspond to ‚ÄúWomen aged 40‚Äì49 working in Finance in Barcelona.‚Äù Even though only the count or average is shown, if that subgroup includes very few individuals, those numbers could reveal private details about them.

Privacy risks in tabular data publishing can arise in two main ways. The first is the external attack: when an outside party uses publicly available information or background knowledge to deduce information about an individual. For instance, if a table reports that there is exactly one person in a specific region and category, an attacker who knows someone matching those conditions can infer that person‚Äôs confidential attribute (such as salary or health status). The second is the internal attack, which can occur even among respondents within the dataset. If two people appear in the same cell and one of them knows their own data, they can infer the other‚Äôs by comparing with the published cell value (for example, if they know the average salary of a two-person group and their own salary, they can deduce the other‚Äôs).

Because these risks depend on the number of individuals contributing to each cell, the cell frequency‚Äîthe count of records behind each aggregate‚Äîis a crucial factor in tabular data protection. When cell frequencies are too small, the cell is considered unsafe. Statistical disclosure control (SDC) techniques aim to protect these cells while keeping the overall table as informative as possible.

The simplest protection strategy is cell suppression, where unsafe cells (those with very small counts or containing sensitive information) are removed or replaced with a symbol such as ‚Äú‚Äì‚Äù or ‚Äúconfidential.‚Äù However, suppression alone can be insufficient because totals and marginal sums may allow suppressed values to be recalculated. To prevent such deductive disclosure, complementary suppression is applied: additional, non-sensitive cells are also suppressed strategically so that the missing values cannot be reconstructed from the published totals. This ensures that every suppressed cell is supported by a set of other suppressed cells that make re-computation impossible.

Another method is controlled rounding, in which numerical values‚Äîsuch as counts or averages‚Äîare rounded to the nearest multiple of a specified base (for example, to the nearest 3 or 5). Rounding introduces small random errors that obscure exact cell frequencies while preserving global consistency and additivity of totals. Controlled tabular adjustment (CTA) is a related approach that makes minimal adjustments to cell values, usually through optimization algorithms, to ensure that sensitive cells deviate from their true values by at least a specified threshold while keeping the table as close as possible to the original.

In some cases, agencies publish interval data rather than exact values. Instead of reporting an exact count or average, they publish ranges such as ‚Äúbetween 3 and 7 individuals‚Äù or ‚Äúaverage income between ‚Ç¨35,000 and ‚Ç¨40,000.‚Äù This technique generalizes results and can be tuned to balance privacy and precision: wider intervals give stronger privacy at the cost of utility.

Noise addition can also be applied to tabular data, though it must be done carefully to preserve statistical consistency. Small random perturbations are introduced to cell values or to the input data before aggregation, ensuring that the overall distribution remains accurate for analysis but that individual contributions cannot be deduced. However, because perturbation introduces distortion, it must be calibrated to maintain the credibility of published statistics‚Äîespecially when the data serve policy or economic purposes.

To evaluate the effectiveness of these techniques, two criteria are commonly used: disclosure risk and information loss (or utility loss). Disclosure risk measures how likely it is that an individual‚Äôs information could be inferred from the published table. Information loss measures how much the modified table deviates from the original, affecting the usefulness of the data for analysis. An ideal anonymization strategy minimizes both‚Äîbut in practice, they are inversely related: increasing protection typically reduces accuracy and vice versa. Agencies therefore aim for a reasonable balance based on the sensitivity of the data and the intended use of the publication.

In the practical workflow of tabular data protection, the process typically begins with identifying key variables‚Äîthose attributes that could serve as quasi-identifiers‚Äîand determining a frequency threshold, such as three or five, below which a cell is considered unsafe. Then, one or more SDC methods (suppression, rounding, or perturbation) are applied. Finally, validation ensures that no suppressed or modified cells can be reconstructed through arithmetic relationships among totals or that noise addition does not excessively distort results.

An important aspect discussed in this session is that anonymizing tabular data is not the same as anonymizing microdata. In tabular data, individuals are already aggregated, so the primary goal is not to protect individual records directly but to prevent inference from aggregated statistics. The techniques, therefore, operate at the aggregate level‚Äîcontrolling how much information each published value reveals about the underlying data.

Ultimately, tabular data protection exemplifies one of the earliest and most classical forms of statistical disclosure control. It demonstrates the principle that even aggregate data can leak sensitive information if not handled carefully. By applying methods such as suppression, rounding, perturbation, and interval publication, data controllers can ensure that the results of statistical analyses remain useful for society while respecting the confidentiality of the individuals whose data underlie those numbers.

### Queryable

P5 ‚Äì Queryable Databases and Differential Privacy

After discussing the protection of tabular data, which focuses on publishing static, aggregated statistics, this session turns to the more dynamic scenario of queryable databases‚Äîsystems that do not publish raw or aggregated data directly but instead allow users to make statistical queries through controlled interfaces, such as APIs or query engines. These databases are increasingly common in government, research, and commercial environments because they promise flexibility: external analysts or applications can request statistics or summaries without the data ever leaving the controller‚Äôs server. However, this model still poses serious privacy challenges. Even when individual records are not exposed, the answers to queries can leak sensitive information, especially if users issue cleverly designed or repeated queries.

A queryable database typically consists of three entities:

A data controller or curator, who collects and stores data from respondents or participants (for example, through surveys, services, or sensors).

A dataset, often containing personal information, organized in relational form.

A user, who can submit statistical queries‚Äîsuch as counts, sums, or averages‚Äîvia an interface, often programmatically.

The central privacy objective is to protect the respondents, not the users. That is, the system must ensure that queries cannot reveal confidential information about individuals whose data is stored in the database.

Privacy Risks in Queryable Databases

Even seemingly harmless queries can compromise privacy when their conditions isolate small subsets of individuals. For instance, if a query asks, ‚ÄúHow many employees in the finance sector in Tarragona earn more than ‚Ç¨100,000?‚Äù and the answer is ‚Äú1,‚Äù an attacker might infer both the existence and the salary of that single individual. If follow-up queries refine conditions (for example, by changing filters or combining results), they can extract additional details. This risk increases when users collude or use external information to link responses with real-world identities.

Two main forms of disclosure can occur:

Identity disclosure, when a query indirectly reveals who an individual is.

Attribute disclosure, when the value of a confidential attribute (such as salary or diagnosis) can be inferred for an identified person.

Unlike static table publication, protecting a queryable database is an ongoing process: each new query potentially leaks more information. Thus, privacy protection mechanisms must monitor query patterns over time and enforce limits dynamically.

Three Families of Protection Techniques

Three broad strategies are used to protect queryable databases: query restriction, query camouflage (generalization), and query perturbation. Each offers a different balance between privacy, computational cost, and result accuracy.

Query Restriction
The most straightforward method is to simply refuse to answer queries that could compromise privacy. The system computes the query set size‚Äîthe number of individual records that match the query conditions‚Äîand denies responses when that number falls below a predefined threshold. For example, the system might only answer queries that affect at least three or five individuals. This prevents small-group disclosures and is analogous to cell suppression in tabular data.

The advantage of query restriction is that it preserves exact accuracy: all returned results are true and unmodified. However, it is extremely demanding to implement securely. The curator must not only monitor each query but also consider query history‚Äîbecause multiple queries can be combined to extract sensitive information even if each, individually, is safe. Keeping track of overlaps between query sets (and potential collusion between users) is computationally expensive, especially for large datasets or open systems with many users. Therefore, while conceptually simple, query restriction scales poorly in practice.

Query Camouflage (Interval or Generalized Responses)
Another strategy is to generalize the answers by reporting intervals instead of precise values. For example, instead of returning that the average salary is ‚Ç¨42,530, the system might report that it lies between ‚Ç¨40,000 and ‚Ç¨45,000. This method provides bounded uncertainty without adding random noise, which preserves logical consistency and allows unlimited querying.

Each confidential attribute can have its own privacy bound‚Äîa parameter defining how wide the intervals should be. A larger interval offers stronger privacy but reduces the utility of the data, since results become less specific. Because interval reporting does not distort the data but rather abstracts it, it is considered a non-perturbative method. However, as with other generalization approaches, it cannot fully prevent inference when users accumulate many queries with overlapping conditions.

Query Perturbation (Noise Addition)
The most powerful and conceptually advanced approach is to add controlled noise to query responses so that individual contributions are hidden, while overall statistical patterns remain accurate. Noise can be introduced at two stages:

Input perturbation, where random noise is added to the raw data before any queries are processed.

Output perturbation, where noise is added to the query result itself before returning it to the user.

Input perturbation provides consistent privacy guarantees across queries but permanently modifies the dataset. Output perturbation preserves the dataset but requires generating new noise for each query, raising challenges for consistency and cumulative leakage.

Differential Privacy

The theoretical foundation for query perturbation is differential privacy (DP)‚Äîa mathematically rigorous model introduced in the mid-2000s and now widely used in both research and industry. Differential privacy formalizes the idea that the inclusion or exclusion of a single individual‚Äôs data should not significantly affect the result of any analysis.

More precisely, a randomized algorithm satisfies differential privacy if, for any two datasets that differ in only one individual, the probability of obtaining a particular output is nearly the same. The difference between these probabilities is controlled by a parameter called epsilon (Œµ), the privacy budget. A smaller Œµ provides stronger privacy (less sensitivity to any individual record) but introduces more noise and therefore reduces accuracy.

In practical terms, differential privacy gives a quantifiable, provable privacy guarantee that does not depend on what external information attackers might possess. It limits what can be inferred about any individual, even in the worst-case scenario. This makes it fundamentally stronger than heuristic approaches like suppression or generalization.

The implementation of differential privacy usually involves adding random noise drawn from a specific probability distribution‚Äîmost commonly Laplace or Gaussian‚Äîto query outputs. The amount of noise depends on the sensitivity of the query function (how much a single individual‚Äôs data can change the result) and on the chosen privacy parameter Œµ. For example, a simple count query has sensitivity 1 (each individual can change the count by at most one), whereas averages or sums have higher sensitivity.

Differential privacy also introduces the concept of a privacy budget over multiple queries. Each query consumes part of the available Œµ; when the total budget is exhausted, no further queries can be answered without violating the overall privacy guarantee. This ensures that repeated querying cannot gradually erode privacy protection.

This model has become the de facto standard for privacy-preserving analytics. It has been adopted in high-profile deployments such as the U.S. Census Bureau‚Äôs 2020 census, which used differential privacy to protect published statistics, and by major technology companies like Apple, Google, and Meta for data collection and analysis of user behavior.

The Randomized Response Analogy

To make the intuition behind differential privacy more accessible, the lecture introduced an older statistical method called randomized response, developed in the 1960s for social science surveys. In randomized response, participants answer sensitive yes/no questions (for example, ‚ÄúHave you ever used illegal drugs?‚Äù) using a randomizing device such as a coin flip. If the coin lands heads, the respondent answers truthfully; if tails, they answer ‚Äúyes‚Äù regardless of the truth. Because the interviewer cannot know whether any given answer is truthful, the individual‚Äôs privacy is protected. Yet, because the probability of randomness is known, the overall proportion of ‚Äúyes‚Äù answers in the population can still be estimated statistically.

Differential privacy generalizes this idea to databases: by adding controlled randomness to query results, it protects individuals‚Äô data while still allowing accurate aggregate statistics to be derived.

Conclusion

Queryable databases illustrate a fundamental trade-off between data utility and individual privacy. Restriction methods guarantee perfect accuracy but are computationally and operationally costly. Generalization provides deterministic uncertainty but can still be circumvented through repeated queries. Noise addition‚Äîespecially as formalized by differential privacy‚Äîoffers strong, quantifiable protection with manageable utility loss, making it the most promising modern approach.

This model marks a conceptual shift from heuristic anonymization toward mathematically provable privacy. Rather than focusing on hiding or deleting specific identifiers, differential privacy ensures that the participation of any individual has an almost negligible effect on analytical outcomes. It is, therefore, the cornerstone of contemporary privacy-preserving data analytics and the natural evolution of statistical disclosure control in the age of big data and machine learning

### Microdata

Microdata represents the most granular and detailed form of structured data: each record corresponds to an individual entity (a person, household, or organization), and each attribute describes some aspect of that entity‚Äîsuch as age, income, education, or health condition. Because microdata preserves individual-level information, it provides great analytical value for research and policymaking, but it also poses the highest privacy risks. Even after removing direct identifiers, individuals can often be re-identified by combining quasi-identifiers with external datasets. Microdata anonymization therefore seeks to transform such datasets to make re-identification or attribute inference practically impossible, while maintaining as much data utility as possible for statistical and analytical purposes.

Types of Attributes and Privacy Threats

As in previous discussions, attributes in microdata can be categorized into several classes:

Direct identifiers, such as name, ID number, or email, which uniquely identify an individual and must always be removed or masked.

Quasi-identifiers, which do not identify a person on their own but can do so in combination with other attributes (for example, age, ZIP code, and gender).

Confidential (or sensitive) attributes, which contain private information that should not be disclosed, such as salary, disease, or political affiliation.

Non-confidential attributes, which are neutral and generally harmless to share.

The main threats to privacy in microdata publication are re-identification (linking records to known individuals) and attribute disclosure (inferring sensitive information about an identified individual). These risks arise because even a few quasi-identifiers can uniquely characterize many people. A classic example is the discovery that 87% of Americans could be uniquely identified by the combination of ZIP code, gender, and date of birth.

Non-Perturbative vs. Perturbative Approaches

Anonymization methods for microdata are broadly divided into non-perturbative and perturbative techniques.

Non-perturbative methods preserve the truthfulness of data values but reduce their precision or granularity. This includes recoding, generalization, and suppression. For example, instead of publishing exact ages, data may be grouped into ranges (20‚Äì29, 30‚Äì39, etc.), or small geographic areas may be aggregated into larger regions. The idea is to ensure that each record becomes indistinguishable from several others with respect to the quasi-identifiers.

Perturbative methods, on the other hand, modify data values by introducing controlled randomness. This includes noise addition (slightly altering numerical values), data swapping (exchanging attribute values between records), rank swapping (reordering values among similar records), and synthetic data generation (creating entirely artificial records that preserve statistical distributions). These methods are particularly useful when precise micro-level accuracy is not required but overall statistical properties must remain valid.

Both categories aim to balance two competing objectives: minimizing disclosure risk and preserving analytical utility. The optimal choice depends on the purpose of the data release.

k-Anonymity: The Foundational Model

The most influential formal model for microdata anonymization is k-anonymity, introduced by Samarati and Sweeney in the late 1990s. A dataset satisfies k-anonymity if every combination of quasi-identifiers appears in at least k records. In other words, each individual is indistinguishable from at least k‚Äì1 others with respect to the identifying attributes. This means that the probability of correctly re-identifying any record is at most 1/k.

To achieve k-anonymity, the dataset is partitioned into equivalence classes, where each class contains at least k records that share the same generalized values for their quasi-identifiers. For example, if a dataset achieves 4-anonymity, each unique combination of age range, gender, and ZIP code must appear in at least four records. The process typically involves iterative generalization (grouping categorical values, widening numerical intervals) and suppression (removing overly unique records).

While k-anonymity provides a clear and intuitive privacy guarantee, it has limitations. It protects only against identity disclosure, not attribute disclosure. If all individuals in an equivalence class share the same value for a sensitive attribute‚Äîsay, they all have the same medical diagnosis‚Äîthen knowing that a person belongs to that group still reveals their confidential information. To address this, several stronger models were developed.

l-Diversity and t-Closeness: Beyond Identity Protection

l-Diversity extends k-anonymity by requiring that each equivalence class contain at least l well-represented values for every sensitive attribute. This ensures diversity within groups, reducing the risk of attribute disclosure. For instance, if an equivalence class of four records all indicate ‚ÄúHIV-positive,‚Äù the dataset is technically 4-anonymous but not diverse; an attacker would immediately learn that anyone in that group has HIV. Ensuring l-diversity means that sensitive attributes vary across the records in each group.

However, l-diversity can still fail when the overall distribution of a sensitive attribute is highly skewed. If one value (e.g., ‚Äúflu‚Äù) dominates globally, then even a diverse group might not protect well. To address this, t-closeness was introduced. It requires that the distribution of a sensitive attribute within each equivalence class be close (within a distance t) to its distribution in the entire dataset. The smaller t is, the stronger the privacy guarantee. This model directly measures how much information about sensitive attributes can be inferred from group membership, ensuring that each group reflects the overall population distribution.

These models form a hierarchy of protection:

k-anonymity prevents identity disclosure.

l-diversity reduces attribute disclosure by ensuring variety.

t-closeness controls how much the internal distribution can deviate from the global one.

Practical Techniques: Masking and Recoding

In practical applications, microdata anonymization is often achieved through masking techniques that modify quasi-identifiers according to chosen privacy parameters. Common strategies include:

Global recoding, where values are replaced with broader categories (e.g., merging job titles into sectors).

Local suppression, where certain values are removed or replaced with missing entries in specific records that threaten anonymity.

Top- and bottom-coding, which cap extreme numerical values‚Äîfor example, reporting all incomes above ‚Ç¨200,000 simply as ‚Äú‚â•200,000.‚Äù

Microaggregation, which replaces groups of similar records with group averages, commonly used for numerical data.

These transformations are carefully evaluated to ensure they maintain the statistical relationships necessary for analysis‚Äîsuch as means, variances, and correlations‚Äîwhile satisfying k-anonymity or its extensions.

Evaluating Risk and Utility

Every anonymization process involves measuring disclosure risk (the likelihood of re-identification) and information loss (the reduction in data quality or analytical power). Techniques such as risk‚Äìutility maps are used to visualize the trade-off between these two dimensions. Analysts can adjust parameters (e.g., the value of k or the granularity of generalization) to achieve the desired balance.

An additional consideration is the context of data release. For public data releases, stronger privacy (higher k or t) is required; for controlled research environments where access is restricted and monitored, weaker anonymization may suffice to retain utility.

Conceptual Transition: Toward Queryable and Differential Privacy

The anonymization of microdata bridges traditional statistical disclosure control and the more recent paradigm of differential privacy. While k-anonymity and its variants rely on modifying data before release, differential privacy introduces randomness during query processing. The former alters data values; the latter perturbs outputs. Both share the same goal‚Äîlimiting how much information about any individual can be inferred‚Äîbut differ in mathematical foundations and operational approach.

Microdata anonymization remains central in domains like healthcare, official statistics, and social research, where tabular or aggregate data is insufficient and researchers require individual-level information. It embodies the classic tension between privacy protection and scientific value, reminding us that the act of anonymization is not merely technical but also ethical: protecting individuals while preserving knowledge that can benefit society.