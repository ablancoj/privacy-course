
<!DOCTYPE html>


<html lang="en" data-content_root="./" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>Privacy in machine learning &#8212; Privacy Book</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="_static/styles/theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="_static/styles/bootstrap.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="_static/styles/pydata-sphinx-theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />

  
  <link href="_static/vendor/fontawesome/6.5.2/css/all.min.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.5.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.5.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.5.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="_static/pygments.css?v=03e43079" />
    <link rel="stylesheet" type="text/css" href="_static/styles/sphinx-book-theme.css?v=eba8b062" />
    <link rel="stylesheet" type="text/css" href="_static/togglebutton.css?v=13237357" />
    <link rel="stylesheet" type="text/css" href="_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="_static/mystnb.8ecb98da25f57f5357bf6f572d296f466b2cfe2517ffebfabe82451661e28f02.css?v=6644e6bb" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-thebe.css?v=4fa983c6" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-design.min.css?v=95c83b7e" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b" />
<link rel="preload" as="script" href="_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b" />
  <script src="_static/vendor/fontawesome/6.5.2/js/all.min.js?digest=dfe6caa3a7d634c4db9b"></script>

    <script src="_static/documentation_options.js?v=9eb32ce0"></script>
    <script src="_static/doctools.js?v=9a2dae69"></script>
    <script src="_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="_static/copybutton.js?v=f281be69"></script>
    <script src="_static/scripts/sphinx-book-theme.js?v=887ef09a"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="_static/togglebutton.js?v=4a39c7ea"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="_static/design-tabs.js?v=f930bc37"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script async="async" src="_static/sphinx-thebe.js?v=c100c467"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'privacy-ml';</script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="Salem paper attacks" href="notebooks/LipariSC_MIA.html" />
    <link rel="prev" title="Privacy in unstructured data" href="unstructured.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-primary-sidebar-checkbox"/>
  <label class="overlay overlay-primary" for="pst-primary-sidebar-checkbox"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-secondary-sidebar-checkbox"/>
  <label class="overlay overlay-secondary" for="pst-secondary-sidebar-checkbox"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>

  
    <header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  
    
  

<a class="navbar-brand logo" href="intro.html">
  
  
  
  
  
    
    
      
    
    
    <img src="_static/logo_v01-original.png" class="logo__image only-light" alt="Privacy Book - Home"/>
    <script>document.write(`<img src="_static/logo_v01-original.png" class="logo__image only-dark" alt="Privacy Book - Home"/>`);</script>
  
  
</a></div>
        <div class="sidebar-primary-item">

 <script>
 document.write(`
   <button class="btn search-button-field search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
    <span class="search-button__default-text">Search</span>
    <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
   </button>
 `);
 </script></div>
        <div class="sidebar-primary-item"><nav class="bd-links bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="intro.html">
                    Welcome to your Jupyter Book
                </a>
            </li>
        </ul>
        <ul class="current nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="intro-gdpr.html">Introduction and GDPR</a></li>
<li class="toctree-l1"><a class="reference internal" href="privacy-by-design.html">Privacy by design</a></li>
<li class="toctree-l1"><a class="reference internal" href="pets.html">Privacy enhancing techniques</a></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="databases.html">Privacy in databases</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="anonymization-library.html">Anonymization library</a></li>
<li class="toctree-l2"><a class="reference internal" href="notebooks/test_anonymization.html">Anonymization examples</a></li>
</ul>
</details></li>
<li class="toctree-l1"><a class="reference internal" href="unstructured.html">Privacy in unstructured data</a></li>
<li class="toctree-l1 current active has-children"><a class="current reference internal" href="#">Privacy in machine learning</a><details open="open"><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="notebooks/LipariSC_MIA.html">Salem paper attacks</a></li>
<li class="toctree-l2"><a class="reference internal" href="notebooks/FederatedLearning-sklearn.html">Federated Learning and Label Flipping attacks</a></li>





<li class="toctree-l2"><a class="reference internal" href="notebooks/unlearning-CIFAR10.html">NeurIPS 2023 Machine Unlearning Challenge Starting Kit</a></li>



</ul>
</details></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><button class="sidebar-toggle primary-toggle btn btn-sm" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</button></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-source-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Source repositories">
    <i class="fab fa-github"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://github.com/ablancoj/privacy-course" target="_blank"
   class="btn btn-sm btn-source-repository-button dropdown-item"
   title="Source repository"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="btn__text-container">Repository</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/ablancoj/privacy-course/issues/new?title=Issue%20on%20page%20%2Fprivacy-ml.html&body=Your%20issue%20content%20here." target="_blank"
   class="btn btn-sm btn-source-issues-button dropdown-item"
   title="Open an issue"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="btn__text-container">Open issue</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="_sources/privacy-ml.md" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.md</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<script>
document.write(`
  <button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="theme-switch fa-solid fa-sun fa-lg" data-mode="light"></i>
    <i class="theme-switch fa-solid fa-moon fa-lg" data-mode="dark"></i>
    <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"></i>
  </button>
`);
</script>


<script>
document.write(`
  <button class="btn btn-sm pst-navbar-icon search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
  </button>
`);
</script>
<button class="sidebar-toggle secondary-toggle btn btn-sm" title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</button>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Privacy in machine learning</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#machine-learning-pipeline">Machine learning pipeline</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#threats">Threats</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#threat-model">Threat model</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#mia">MIA</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#connection-to-overfitting">Connection to overfitting</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#attribute-inference-attacks">Attribute inference attacks</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#reconstruction-attacks">Reconstruction attacks</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#approximate-differential-privacy">Approximate differential privacy</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#machine-unlearning">Machine unlearning</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#federated-learning">Federated learning</a></li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <section class="tex2jax_ignore mathjax_ignore" id="privacy-in-machine-learning">
<h1>Privacy in machine learning<a class="headerlink" href="#privacy-in-machine-learning" title="Link to this heading">#</a></h1>
<section id="machine-learning-pipeline">
<h2>Machine learning pipeline<a class="headerlink" href="#machine-learning-pipeline" title="Link to this heading">#</a></h2>
<p><img alt="alt text" src="_images/ml_pipeline.png" /></p>
</section>
<section id="threats">
<h2>Threats<a class="headerlink" href="#threats" title="Link to this heading">#</a></h2>
<p>Several studies, such as Shokri et al. (2017), among many others, have shown that trained ML models may leak personal information from the data used to train them.</p>
<ul class="simple">
<li><p>Membership inference attacks</p></li>
<li><p>Attribute inference attacks</p></li>
<li><p>Reconstruction attacks</p></li>
</ul>
<p><img alt="alt text" src="_images/neural_network.png" /></p>
<p>While we will mostly refer to neural networks and deep learning most of the privacy issues described apply to other models.
Most classification models output the decisions’ confidence.
Other models directly leak private information, such as kNN and SVMs, which include some (or all) training data as part of the model.
DP techniques easily apply to differentiable model training (e.g., SGD).</p>
<section id="threat-model">
<h3>Threat model<a class="headerlink" href="#threat-model" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p>Type of access to the model</p>
<ul>
<li><p>Black-box access → prediction probabilities, logits in DL.</p></li>
<li><p>White-box access → weights, activations, gradients during training, known architecture.</p></li>
</ul>
</li>
<li><p>Learning architecture</p>
<ul>
<li><p>Centralized learning → published trained model, API-only access.</p></li>
<li><p>Federated learning → open to white-box attacks</p></li>
</ul>
</li>
<li><p>Knowledge of data distribution</p>
<ul>
<li><p>Availability of public data, synthetic data, correlations in attributes, etc.</p></li>
</ul>
</li>
</ul>
</section>
</section>
<section id="mia">
<h2>MIA<a class="headerlink" href="#mia" title="Link to this heading">#</a></h2>
<p>MIAs aim to determine whether a data point was present in the training data used to build a model.
Given a point in the distribution and a trained model, the attack returns 1 bit of information.</p>
<p><img alt="alt text" src="_images/mia.png" /></p>
<p>Although this may not at first seem to pose a serious privacy risk, the threat is clear in settings such as health analytics where the distinction between case and control groups could reveal an individual’s sensitive conditions.
Successful MIAs open the door to other attacks, such as attribute inference and reconstruction attacks.
They can also be used to detect copyright violations or to measure unlearning success.</p>
<p><em>Experiment</em> (Membership Experiment <span class="math notranslate nohighlight">\(Exp^M(\mathcal{A},A,n,\mathcal{D})\)</span>).
Let <span class="math notranslate nohighlight">\(\mathcal{A}\)</span> be an adversary, <span class="math notranslate nohighlight">\(A\)</span> a learning algorithm, <span class="math notranslate nohighlight">\(n\)</span> be a positive integer, and <span class="math notranslate nohighlight">\(\mathcal{D}\)</span> be a distribution over data points <span class="math notranslate nohighlight">\((x,y)\)</span>. The membership experiment proceeds as follows:</p>
<ol class="arabic simple">
<li><p>Sample <span class="math notranslate nohighlight">\(S \sim \mathcal{D}^n\)</span>, and let $A_S = A(S).</p></li>
<li><p>Choose <span class="math notranslate nohighlight">\(b \leftarrow \{0,1\}\)</span> uniformly at random.</p></li>
<li><p>Draw <span class="math notranslate nohighlight">\(z \sim S\)</span> if <span class="math notranslate nohighlight">\(b=0\)</span>, or <span class="math notranslate nohighlight">\(z \sim D\)</span> if <span class="math notranslate nohighlight">\(b=1\)</span>.</p></li>
<li><p><span class="math notranslate nohighlight">\(Exp^M(\mathcal{A},A,n,\mathcal{D})\)</span> is <span class="math notranslate nohighlight">\(1\)</span> if <span class="math notranslate nohighlight">\(\mathcal{A}(z, A_S, n, \mathcal{D}) = b\)</span> and 0 otherwise. <span class="math notranslate nohighlight">\(\mathcal{A}\)</span> must output either <span class="math notranslate nohighlight">\(0\)</span> or <span class="math notranslate nohighlight">\(1\)</span>.</p></li>
</ol>
<p><em>Membership advantage</em>. The membership advantage of <span class="math notranslate nohighlight">\(\mathcal{A}\)</span> is defined as</p>
<div class="math notranslate nohighlight">
\[Adv^M(\mathcal{A}, A, n, \mathcal{D}) = 2 \Pr[Exp^M(\mathcal{A}, A, n, \mathcal{D})=1]-1\]</div>
<p>where probabilities are taken over the coin flips of <span class="math notranslate nohighlight">\(\mathcal{A}\)</span>, the random choices of <span class="math notranslate nohighlight">\(S\)</span> and <span class="math notranslate nohighlight">\(b\)</span>, and the random data point <span class="math notranslate nohighlight">\(z \sim S\)</span> or <span class="math notranslate nohighlight">\(z \sim D\)</span>.</p>
<p>Equivalently, the right-hand side can be expressed as the difference between <span class="math notranslate nohighlight">\(\mathcal{A}\)</span>’s true and false positive rates</p>
<div class="math notranslate nohighlight">
\[ Adv^M = \Pr[\mathcal{A}=0|b=0] - \Pr[\mathcal{A}=0|b=1]\]</div>
<p>where <span class="math notranslate nohighlight">\(Adv^M\)</span> is shortcut for <span class="math notranslate nohighlight">\(Adv^M(\mathcal{A}, A, n, \mathcal{D})\)</span>.</p>
<section id="connection-to-overfitting">
<h3>Connection to overfitting<a class="headerlink" href="#connection-to-overfitting" title="Link to this heading">#</a></h3>
<p>Overfitting has been shown to predict attacker advantage (Yeom et al. 2018).
In black-box attacks, prediction probabilities (for any classifier) or prediction loss are used to determine membership.
Models, especially when overfit to the training data, display different behavior when encountered with previously seen data.</p>
<p><img alt="alt text" src="_images/mia_overfitting.png" /></p>
<p><video src="_images/evo_non_overfit.mp4" title="alt text"><a href="_images/evo_non_overfit.mp4">alt text</a></video></p>
<p><video src="_images/evo_overfit.mp4" title="alt text"><a href="_images/evo_overfit.mp4">alt text</a></video></p>
<p><img alt="alt text" src="_images/salem.png" /></p>
</section>
</section>
<section id="attribute-inference-attacks">
<h2>Attribute inference attacks<a class="headerlink" href="#attribute-inference-attacks" title="Link to this heading">#</a></h2>
<p>In an attribute inference attack, the adversary uses a machine learning model and incomplete information about a data point to infer missing information.
For example, the adversary is given partial information about an individual’s medical record and attempts to infer the individual’s genotype by using a model trained on similar medical records.
Can be obtained from successful MIA.</p>
<p><img alt="alt text" src="_images/aia.png" /></p>
</section>
<section id="reconstruction-attacks">
<h2>Reconstruction attacks<a class="headerlink" href="#reconstruction-attacks" title="Link to this heading">#</a></h2>
<p>Reconstruction or model inversion attacks attempt to build the whole training dataset from the information leaked by the trained model.
Can also be obtained from MIAs.
Often use GANs.</p>
<p><img alt="alt text" src="_images/reconstruction.png" /></p>
</section>
<section id="approximate-differential-privacy">
<h2>Approximate differential privacy<a class="headerlink" href="#approximate-differential-privacy" title="Link to this heading">#</a></h2>
<p>Let <span class="math notranslate nohighlight">\(\epsilon\)</span> be a positive real number and <span class="math notranslate nohighlight">\(\mathcal{A}\)</span> be a randomized algorithm that takes a dataset as input. Let  <span class="math notranslate nohighlight">\(im \mathcal{A}\)</span> denote the image of <span class="math notranslate nohighlight">\(\mathcal{A}\)</span>. The algorithm <span class="math notranslate nohighlight">\(\mathcal{A}\)</span> is said to provide <span class="math notranslate nohighlight">\((\epsilon,\delta)\)</span>-differential privacy if, for all datasets <span class="math notranslate nohighlight">\(D_1\)</span> and <span class="math notranslate nohighlight">\(D_2\)</span> that differ on a single element (i.e., the data of one person), and all subsets <span class="math notranslate nohighlight">\(S \subset im \mathcal{A}\)</span>:</p>
<div class="math notranslate nohighlight">
\[\Pr[\mathcal{A}(D_1) \in S] \le e^{\epsilon}\Pr[\mathcal{A}(D_2) \in S]+\delta \]</div>
<p>where the probability is taken over the randomness used by the algorithm.</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(\epsilon\)</span> should be close to <span class="math notranslate nohighlight">\(0\)</span> and <span class="math notranslate nohighlight">\(\delta &lt;&lt; 1/|D|\)</span></p></li>
<li><p>Sequential composition: applying mechanisms that are (<span class="math notranslate nohighlight">\(\epsilon_i,\delta_i)\)</span>-differentially private results in <span class="math notranslate nohighlight">\((\sum \epsilon_i, \sum \delta_i)\)</span>-differential privacy.</p></li>
<li><p>Resistance to post-processing: no processing done on data released by a differentially private mechanism will degrade the privacy guarantee.</p></li>
<li><p><span class="math notranslate nohighlight">\((\epsilon,\delta)\)</span>-DP is often achieved via the Gaussian mechanism:</p></li>
</ul>
<div class="math notranslate nohighlight">
\[ \mathcal{M}_{Gauss} = f(x) + \mathcal{M}\left(0, \frac{2\ln(1.25/\delta)(\Delta f)^2}{\epsilon^2} \right) \]</div>
<ul class="simple">
<li><p>Differential privacy bounds the probability of identifying any individual record within a database, parametrized by <span class="math notranslate nohighlight">\(\epsilon\)</span>.</p></li>
<li><p>Results of a DP function are unaltered by the presence or absence of any single record.</p></li>
<li><p>This looks ideal to protect against MIAs in ML.</p></li>
<li><p><span class="math notranslate nohighlight">\(\epsilon\)</span> closer to 0 provide more privacy at the cost of less data utility.</p></li>
<li><p>Relaxations to DP attempt to mitigate this.</p></li>
<li><p>In ML, <span class="math notranslate nohighlight">\((\epsilon,\delta)\)</span>-DP has been applied to stochastic gradient descent, together with the moments accountant.</p></li>
</ul>
<p><img alt="alt text" src="_images/dpsgd.png" /></p>
<p>Yeom provides a bound for the attacker’s advantage in DPML:</p>
<div class="math notranslate nohighlight">
\[Adv \le e^\epsilon - 1\]</div>
<p><span class="math notranslate nohighlight">\(Adv=1\)</span> implies privacy is broken. Therefore, the bound is irrelevant for:</p>
<div class="math notranslate nohighlight">
\[ \epsilon \ge \ln{⁡2} \approx 0.7 \]</div>
<p><img alt="alt text" src="_images/dp_bound.png" /></p>
<ul class="simple">
<li><p>DP-SGD introduces several new hyperparameters to optimize: <span class="math notranslate nohighlight">\(\sigma\)</span>, <span class="math notranslate nohighlight">\(\epsilon\)</span>, <span class="math notranslate nohighlight">\(\delta\)</span>, <span class="math notranslate nohighlight">\(q=L/N\)</span>, <span class="math notranslate nohighlight">\(C\)</span>, <span class="math notranslate nohighlight">\(T\)</span>, which are interdependent.</p></li>
<li><p>To correctly compute the noise to add and not lose additional utility, it should be computed on a per example basis, thus losing most of the benefits of parallelization</p></li>
<li><p>Training times increase of 10x—50x.</p></li>
<li><p>Relaxations of DP make the privacy guarantees even less clear, e.g., the moments accountant.</p></li>
<li><p>High accuracy loss, especially if choosing adequate budgets.</p></li>
</ul>
<p><img alt="alt text" src="_images/dpfy1.png" />
<img alt="alt text" src="_images/dpfy2.png" />
<img alt="alt text" src="_images/dpfy3.png" /></p>
</section>
<section id="machine-unlearning">
<h2>Machine unlearning<a class="headerlink" href="#machine-unlearning" title="Link to this heading">#</a></h2>
<p>An unlearning algorithm takes as input a pre-trained model and one or more samples from the train set to unlearn (the “forget set”).
From the model, forget set, and retain set, the unlearning algorithm produces an updated model.
An ideal unlearning algorithm produces a model that is indistinguishable from the model trained without the forget set.
MIA advantage used as a metric.</p>
<p><img alt="alt text" src="_images/unlearning.png" /></p>
</section>
<section id="federated-learning">
<h2>Federated learning<a class="headerlink" href="#federated-learning" title="Link to this heading">#</a></h2>
<p>Federated learning (FL) is a machine learning setting where many clients (e.g., mobile devices or whole organizations) collaboratively train a model under the orchestration of a central server, while keeping the training data decentralized.
FL embodies the principles of focused data collection, minimization, separation, and aggregation and can mitigate many of the systemic privacy risks and costs resulting from traditional, centralized machine learning and data science approaches.</p>
<p><img alt="alt text" src="_images/fl.png" /></p>
<p>While FL helps with privacy, it still has some issues to consider:
Privacy issues → White-box access to clients’ contributions.
Security issues → Byzantine, poisoning, adversarial attacks.
Homomorphic encryption, secure multiparty computation, differential privacy at the client level, etc. have been used to tackle some of the privacy issues.
Outlier and anomaly detection mechanisms can be used to detect security attacks.
Some protections against these issues have a negative effect on the other.</p>
<p><img alt="alt text" src="_images/homo-fl.png" /></p>
<p>Secure aggregation protects against a honest-but-curious model manager.
Still, MIAs, AIAs, and reconstruction attacks are still feasible on the aggregated model.
Secure aggregation may prevent the model manager from protecting against security attacks (e.g., byzantine &amp; poisoning attacks).</p>
</section>
<div class="toctree-wrapper compound">
</div>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./."
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
              
              
                <footer class="prev-next-footer d-print-none">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="unstructured.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title">Privacy in unstructured data</p>
      </div>
    </a>
    <a class="right-next"
       href="notebooks/LipariSC_MIA.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Salem paper attacks</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">


  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#machine-learning-pipeline">Machine learning pipeline</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#threats">Threats</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#threat-model">Threat model</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#mia">MIA</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#connection-to-overfitting">Connection to overfitting</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#attribute-inference-attacks">Attribute inference attacks</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#reconstruction-attacks">Reconstruction attacks</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#approximate-differential-privacy">Approximate differential privacy</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#machine-unlearning">Machine unlearning</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#federated-learning">Federated learning</a></li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Alberto Blanco-Justicia
</p>

  </div>
  
  <div class="footer-item">
    

  <p class="copyright">
    
      © Copyright 2023.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b"></script>
<script src="_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>