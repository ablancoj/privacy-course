
<!DOCTYPE html>


<html lang="en" data-content_root="./" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>8. Privacy in machine learning &#8212; Privacy Engineering</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="_static/styles/theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="_static/styles/bootstrap.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="_static/styles/pydata-sphinx-theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />

  
  <link href="_static/vendor/fontawesome/6.5.2/css/all.min.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.5.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.5.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.5.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="_static/pygments.css?v=03e43079" />
    <link rel="stylesheet" type="text/css" href="_static/styles/sphinx-book-theme.css?v=eba8b062" />
    <link rel="stylesheet" type="text/css" href="_static/togglebutton.css?v=13237357" />
    <link rel="stylesheet" type="text/css" href="_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="_static/mystnb.8ecb98da25f57f5357bf6f572d296f466b2cfe2517ffebfabe82451661e28f02.css?v=6644e6bb" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-thebe.css?v=4fa983c6" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-design.min.css?v=95c83b7e" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b" />
<link rel="preload" as="script" href="_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b" />
  <script src="_static/vendor/fontawesome/6.5.2/js/all.min.js?digest=dfe6caa3a7d634c4db9b"></script>

    <script src="_static/documentation_options.js?v=9eb32ce0"></script>
    <script src="_static/doctools.js?v=9a2dae69"></script>
    <script src="_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="_static/copybutton.js?v=f281be69"></script>
    <script src="_static/scripts/sphinx-book-theme.js?v=887ef09a"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="_static/togglebutton.js?v=4a39c7ea"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="_static/design-tabs.js?v=f930bc37"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script async="async" src="_static/sphinx-thebe.js?v=c100c467"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'privacy-ml';</script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="8.9. Salem paper attacks" href="notebooks/LipariSC_MIA.html" />
    <link rel="prev" title="7. Privacy in unstructured data" href="unstructured.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-primary-sidebar-checkbox"/>
  <label class="overlay overlay-primary" for="pst-primary-sidebar-checkbox"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-secondary-sidebar-checkbox"/>
  <label class="overlay overlay-secondary" for="pst-secondary-sidebar-checkbox"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>

  
    <header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  
    
  

<a class="navbar-brand logo" href="intro.html">
  
  
  
  
  
    
    
      
    
    
    <img src="_static/logo_v01-original.png" class="logo__image only-light" alt="Privacy Engineering - Home"/>
    <script>document.write(`<img src="_static/logo_v01-original.png" class="logo__image only-dark" alt="Privacy Engineering - Home"/>`);</script>
  
  
</a></div>
        <div class="sidebar-primary-item">

 <script>
 document.write(`
   <button class="btn search-button-field search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
    <span class="search-button__default-text">Search</span>
    <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
   </button>
 `);
 </script></div>
        <div class="sidebar-primary-item"><nav class="bd-links bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="intro.html">
                    Privacy engineering
                </a>
            </li>
        </ul>
        <ul class="current nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="intro-gdpr.html">1. Foundations of Privacy and Data Protection</a></li>
<li class="toctree-l1"><a class="reference internal" href="privacy-by-design.html">2. Privacy by Design</a></li>
<li class="toctree-l1"><a class="reference internal" href="pets.html">3. Privacy enhancing techniques</a></li>

<li class="toctree-l1"><a class="reference internal" href="databases.html">5. Privacy in databases</a></li>

<li class="toctree-l1"><a class="reference internal" href="unstructured.html">7. Privacy in unstructured data</a></li>
<li class="toctree-l1 current active has-children"><a class="current reference internal" href="#">8. Privacy in machine learning</a><details open="open"><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="notebooks/LipariSC_MIA.html">8.9. Salem paper attacks</a></li>
<li class="toctree-l2"><a class="reference internal" href="notebooks/FederatedLearning-sklearn.html">8.10. Federated Learning and Label Flipping attacks</a></li>





<li class="toctree-l2"><a class="reference internal" href="notebooks/unlearning-CIFAR10.html">8.16. NeurIPS 2023 Machine Unlearning Challenge Starting Kit</a></li>



</ul>
</details></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><button class="sidebar-toggle primary-toggle btn btn-sm" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</button></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-source-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Source repositories">
    <i class="fab fa-github"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://github.com/ablancoj/privacy-course" target="_blank"
   class="btn btn-sm btn-source-repository-button dropdown-item"
   title="Source repository"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="btn__text-container">Repository</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/ablancoj/privacy-course/issues/new?title=Issue%20on%20page%20%2Fprivacy-ml.html&body=Your%20issue%20content%20here." target="_blank"
   class="btn btn-sm btn-source-issues-button dropdown-item"
   title="Open an issue"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="btn__text-container">Open issue</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="_sources/privacy-ml.md" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.md</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<script>
document.write(`
  <button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="theme-switch fa-solid fa-sun fa-lg" data-mode="light"></i>
    <i class="theme-switch fa-solid fa-moon fa-lg" data-mode="dark"></i>
    <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"></i>
  </button>
`);
</script>


<script>
document.write(`
  <button class="btn btn-sm pst-navbar-icon search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
  </button>
`);
</script>
<button class="sidebar-toggle secondary-toggle btn btn-sm" title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</button>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Privacy in machine learning</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#machine-learning-pipeline">8.1. Machine learning pipeline</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#threats">8.2. Threats</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#threat-model">8.2.1. Threat model</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#mia">8.3. MIA</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#connection-to-overfitting">8.3.1. Connection to overfitting</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#attribute-inference-attacks">8.4. Attribute inference attacks</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#reconstruction-attacks">8.5. Reconstruction attacks</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#approximate-differential-privacy">8.6. Approximate differential privacy</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#machine-unlearning">8.7. Machine unlearning</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#federated-learning">8.8. Federated learning</a></li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <section class="tex2jax_ignore mathjax_ignore" id="privacy-in-machine-learning">
<h1><span class="section-number">8. </span>Privacy in machine learning<a class="headerlink" href="#privacy-in-machine-learning" title="Link to this heading">#</a></h1>
<p>Machine learning (ML) has become a cornerstone of modern data processing, powering applications from recommendation systems and medical diagnostics to language models and autonomous vehicles. However, as these systems rely heavily on large volumes of data—often including sensitive personal information—they introduce new vectors of privacy risk. Privacy in machine learning concerns not only the protection of data during training but also the potential leakage of private information through the trained model itself. This session explored the privacy threats across the machine learning lifecycle and the technical approaches developed to mitigate them.</p>
<p>Privacy Risks in the ML Lifecycle</p>
<p>The privacy risks in machine learning can be grouped according to the stages of the model lifecycle:</p>
<p>Data collection and preprocessing – where raw data may contain personally identifiable information (PII) or confidential attributes.</p>
<p>Model training – where data is used to optimize model parameters, and sensitive patterns may be memorized.</p>
<p>Model deployment and inference – where trained models are exposed via APIs or publicly released, creating new avenues for leakage.</p>
<p>At each stage, adversaries can exploit vulnerabilities to infer or reconstruct private information.</p>
<p>During data collection, the classic risks of data privacy still apply: excessive collection, inadequate anonymization, and linkage with external sources can expose individuals. During training, however, new risks emerge. Machine learning algorithms, particularly deep neural networks, have the capacity to memorize parts of the training data, especially when overfitting. This memorization can lead to unintentional leakage of sensitive details, such as names or unique patterns, into the model’s parameters.</p>
<p>Once a model is deployed, privacy threats shift to inference-time attacks. Even without direct access to the data or parameters, attackers can probe the model through queries and exploit statistical signals in its outputs to learn about the underlying data. These are known as model inversion, membership inference, and reconstruction attacks.</p>
<p>Types of Privacy Attacks on Machine Learning Models</p>
<p>Membership Inference Attacks (MIA)
These attacks aim to determine whether a specific data record was part of a model’s training dataset. Given a model and a data point, an attacker observes the model’s confidence or prediction behavior. Typically, models are more confident or accurate on examples they have seen during training, especially if they are overfitted. For instance, an attacker could determine whether a patient’s medical record was used to train a disease classifier, potentially revealing their participation in a sensitive study. This type of attack compromises the confidentiality of training data membership rather than its content.</p>
<p>Attribute Inference Attacks
Here, the goal is to infer unknown or hidden attributes of an individual based on what the model reveals. For example, by observing outputs of a salary prediction model that uses features like education and job title, an attacker might deduce an individual’s missing attribute, such as gender or age, if the model implicitly captures correlations between those variables. Attribute inference attacks exploit the statistical dependencies learned by the model.</p>
<p>Model Inversion and Reconstruction Attacks
These are more aggressive attacks that aim to reconstruct sensitive features or even entire data samples from a trained model. For example, given access to a facial recognition model, an attacker can use gradient-based optimization to reconstruct approximate images of faces that were used in training. Similarly, in language models, it is possible to extract fragments of training data—such as names, addresses, or verbatim text—by prompting the model appropriately. This phenomenon has been demonstrated in large generative models, where memorized sequences from training datasets (e.g., confidential documents or private code) have been unintentionally reproduced.</p>
<p>Data Poisoning and Backdoor Attacks
While not directly a privacy violation, these attacks involve injecting malicious data during training to manipulate model behavior or cause it to leak information under certain triggers. Poisoning can bias the model or embed “backdoors” that allow specific queries to retrieve sensitive information.</p>
<p>The effectiveness of these attacks depends on the attacker’s access level.</p>
<p>In black-box attacks, the attacker can only query the model and observe outputs.</p>
<p>In white-box attacks, the attacker has full access to model parameters, gradients, or training code.
Even black-box attacks, however, can achieve surprisingly high success rates, particularly in overfitted models.</p>
<p>Mitigation Strategies and Privacy-Preserving Techniques</p>
<p>To mitigate these risks, several complementary strategies have been developed. They differ in the level at which they intervene—data, model, or system—and in how they balance privacy protection with model performance.</p>
<p>Data Anonymization and Preprocessing
Before training, sensitive data can be anonymized using traditional methods (such as generalization, suppression, or perturbation) to remove explicit identifiers. However, anonymization alone is insufficient in high-dimensional or unstructured data contexts, since models can still learn to identify individuals indirectly.</p>
<p>Differential Privacy in Model Training
The most rigorous framework for privacy-preserving learning is Differentially Private Stochastic Gradient Descent (DP-SGD). This approach incorporates differential privacy directly into the training process. During each gradient update, the algorithm clips gradients (limiting the influence of any single training example) and adds random noise. The cumulative effect ensures that the model’s parameters reveal almost nothing about any particular record in the training data.</p>
<p>The strength of the guarantee is again controlled by the privacy parameter ε, which quantifies how much information about a single individual can be inferred. Smaller ε means stronger privacy but greater noise, potentially degrading model accuracy. This trade-off must be carefully tuned. DP-SGD is now implemented in major ML frameworks (TensorFlow Privacy, PyTorch Opacus) and has been adopted by companies like Apple and Google for analytics and personalization.</p>
<p>Federated Learning
Federated learning (FL) offers an architectural solution that complements differential privacy. Instead of centralizing all data on a single server, FL keeps data local—on users’ devices or in institutional silos—and sends only model updates (parameter gradients) to a central aggregator. The global model is updated using these aggregated contributions, so raw data never leaves its origin.</p>
<p>While this decentralization enhances privacy, FL is not immune to attacks: model updates can still leak information about the underlying local data. To strengthen protection, secure aggregation protocols and differentially private federated learning are used. Secure aggregation uses cryptographic techniques to ensure that the server only sees the sum of updates, not any individual’s contribution. When combined with DP, federated learning achieves strong privacy while preserving collaborative learning benefits.</p>
<p>Secure Multiparty Computation (SMPC) and Homomorphic Encryption
These cryptographic PETs enable collaborative model training or inference without exposing raw data. In SMPC, multiple parties jointly compute a model (e.g., logistic regression) over their combined datasets, but each keeps its data secret, revealing only encrypted shares of intermediate results. Homomorphic encryption goes further, allowing computations to be performed directly on encrypted data; the model can produce encrypted outputs that only the data owner can decrypt. Although computationally expensive, these methods are increasingly practical for small to medium-sized models.</p>
<p>Model Regularization and Privacy-Aware Design
Simple training practices—such as dropout, early stopping, and model compression—also help reduce memorization, indirectly improving privacy. By preventing overfitting, models are less likely to remember specific training examples. Furthermore, access control and query auditing at the API level can detect suspicious query patterns indicative of inference attacks.</p>
<p>Trade-offs and Limitations</p>
<p>Every privacy-preserving ML approach involves trade-offs. Adding noise (as in differential privacy) reduces accuracy; federated learning increases communication and computation overhead; and encryption-based methods can be prohibitively slow. The appropriate balance depends on context: in healthcare or finance, stronger privacy is prioritized even at the cost of performance; in consumer applications, small privacy budgets may suffice for practical purposes.</p>
<p>Moreover, while differential privacy provides strong mathematical guarantees, it does not protect against all attack types—particularly those exploiting side channels or poor implementation. Similarly, federated learning can protect data locality but not against malicious participants or compromised devices.</p>
<p>The Emerging Concept of Model Privacy</p>
<p>This session concluded by emphasizing that privacy in machine learning extends beyond protecting training data—it also involves protecting the models themselves. Trained models may encode sensitive intellectual property or proprietary insights, which can be stolen through model extraction attacks. Thus, privacy preservation now encompasses both data privacy and model privacy, forming a unified security objective.</p>
<p>Modern privacy-aware ML design integrates multiple layers: data minimization at collection; anonymization or obfuscation during preprocessing; privacy-preserving learning (DP-SGD, FL, SMPC); and monitoring or access control at deployment. These techniques collectively operationalize the principle of privacy by design in artificial intelligence systems—ensuring that privacy is not an afterthought but an inherent property of model architecture and learning algorithms.</p>
<p>As AI systems grow more capable and pervasive, embedding privacy protection at every stage becomes essential—not only to comply with regulation but also to maintain trust in algorithmic decision-making. The field of privacy-preserving machine learning thus represents the convergence of data protection law, cryptography, and statistical learning theory—an evolving discipline at the heart of ethical AI.</p>
<p>Machine Unlearning: Making Models Forget</p>
<p>A particularly challenging aspect of privacy in machine learning arises after a model has been trained. Even if strong privacy-preserving methods are applied during data collection and training, there are scenarios where certain data points must later be deleted — for example, if a user withdraws consent under the GDPR’s “right to be forgotten”, or if sensitive or erroneous records are discovered post-training. In traditional ML pipelines, deleting data from the dataset is straightforward, but ensuring that its influence is erased from the trained model is far more complex. This problem is known as machine unlearning.</p>
<p>The core idea of machine unlearning is to remove the impact of specific data samples from a model’s parameters without retraining from scratch. Naively retraining is theoretically correct — if the model is retrained without the unwanted data, it no longer depends on it — but this approach is computationally infeasible for large-scale models, which may take days or weeks to train. Efficient unlearning aims to achieve a similar effect more quickly, ideally with localized updates or algorithmic adjustments.</p>
<p>There are several strategies for achieving this goal, each with different trade-offs between efficiency, completeness, and scalability:</p>
<p>Exact Unlearning by Retraining Subsets
Some methods retrain only the affected components of the model. For example, in ensemble methods like random forests or gradient boosting, individual trees or weak learners that used the deleted data can be retrained independently. Similarly, in modular neural networks, layers or submodels can be partially retrained to remove specific information. This approach achieves precise unlearning while reducing computational cost compared to full retraining.</p>
<p>Approximate or Certified Unlearning
A more recent line of work seeks formal guarantees that a model behaves as if it had been trained on the reduced dataset, up to a bounded error. One approach uses influence functions, which approximate how much each training point affects model parameters. By inverting these influences, it is possible to “roll back” the effect of a deleted sample. While not exact, this provides quantifiable, provable bounds on residual information.</p>
<p>Gradient-Based Unlearning
In differentiable models such as neural networks, unlearning can be approached by performing gradient ascent on the deleted samples — that is, updating the model in the direction that undoes their contribution. This technique can selectively erase features or patterns associated with the removed data but risks destabilizing the model if not carefully controlled.</p>
<p>Data Partitioning and Sliceable Training
Another promising direction involves designing models that are unlearning-friendly from the outset. Data is divided into partitions or “slices,” and the model tracks which parameters depend on which slice. If data from one partition needs to be forgotten, only the corresponding parameter subset is updated. This design principle, sometimes called machine unlearning by design, aligns directly with privacy by design philosophy — anticipating future deletion requests rather than treating them as afterthoughts.</p>
<p>Privacy, Accountability, and the Right to Be Forgotten</p>
<p>Machine unlearning connects technical ML research directly to data protection law. Under the GDPR, data subjects have the right to have their personal data erased when it is no longer necessary for the purposes for which it was collected or when consent is withdrawn. For machine learning systems that derive knowledge from user data, this creates a profound technical challenge: ensuring that the effects of personal data are eliminated from both stored datasets and trained models.</p>
<p>From a compliance perspective, organizations must be able to demonstrate that a model no longer depends on deleted data — a task that requires verifiable procedures or certified unlearning techniques. This pushes the field toward developing measurable metrics for forgetting effectiveness. Emerging research explores auditing tools that can detect whether a model still contains traces of removed samples, using reverse inference or statistical tests.</p>
<p>In practice, many operational systems today achieve partial compliance by retraining models periodically or by limiting training data retention to short cycles. Others use federated learning architectures, where individual clients can simply stop participating, effectively “forgetting” their data in subsequent rounds. Still, the research goal remains to achieve instant, verifiable unlearning at the model level.</p>
<p>The Future of Privacy-Aware Learning</p>
<p>Machine unlearning represents the next step in the evolution of privacy-preserving machine learning — complementing preventive techniques like differential privacy and federated learning with corrective ones that restore privacy retroactively. In an ideal privacy-aware ecosystem, users could not only trust that their data is protected during training but also exercise ongoing control: contributing, modifying, or removing data at will, with immediate effect on deployed models.</p>
<p>Conceptually, unlearning closes the loop between data protection principles (such as purpose limitation, minimization, and the right to erasure) and technical machine learning practices. It ensures that models remain compliant, ethical, and adaptive to user control throughout their lifecycle. Although efficient unlearning is still an open research challenge—particularly for large deep networks and generative models—it embodies the ultimate vision of privacy by design: systems that can both learn and forget responsibly.</p>
<section id="machine-learning-pipeline">
<h2><span class="section-number">8.1. </span>Machine learning pipeline<a class="headerlink" href="#machine-learning-pipeline" title="Link to this heading">#</a></h2>
<p><img alt="alt text" src="_images/ml_pipeline.png" /></p>
</section>
<section id="threats">
<h2><span class="section-number">8.2. </span>Threats<a class="headerlink" href="#threats" title="Link to this heading">#</a></h2>
<p>Several studies, such as Shokri et al. (2017), among many others, have shown that trained ML models may leak personal information from the data used to train them.</p>
<ul class="simple">
<li><p>Membership inference attacks</p></li>
<li><p>Attribute inference attacks</p></li>
<li><p>Reconstruction attacks</p></li>
</ul>
<p><img alt="alt text" src="_images/neural_network.png" /></p>
<p>While we will mostly refer to neural networks and deep learning most of the privacy issues described apply to other models.
Most classification models output the decisions’ confidence.
Other models directly leak private information, such as kNN and SVMs, which include some (or all) training data as part of the model.
DP techniques easily apply to differentiable model training (e.g., SGD).</p>
<section id="threat-model">
<h3><span class="section-number">8.2.1. </span>Threat model<a class="headerlink" href="#threat-model" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p>Type of access to the model</p>
<ul>
<li><p>Black-box access → prediction probabilities, logits in DL.</p></li>
<li><p>White-box access → weights, activations, gradients during training, known architecture.</p></li>
</ul>
</li>
<li><p>Learning architecture</p>
<ul>
<li><p>Centralized learning → published trained model, API-only access.</p></li>
<li><p>Federated learning → open to white-box attacks</p></li>
</ul>
</li>
<li><p>Knowledge of data distribution</p>
<ul>
<li><p>Availability of public data, synthetic data, correlations in attributes, etc.</p></li>
</ul>
</li>
</ul>
</section>
</section>
<section id="mia">
<h2><span class="section-number">8.3. </span>MIA<a class="headerlink" href="#mia" title="Link to this heading">#</a></h2>
<p>MIAs aim to determine whether a data point was present in the training data used to build a model.
Given a point in the distribution and a trained model, the attack returns 1 bit of information.</p>
<p><img alt="alt text" src="_images/mia.png" /></p>
<p>Although this may not at first seem to pose a serious privacy risk, the threat is clear in settings such as health analytics where the distinction between case and control groups could reveal an individual’s sensitive conditions.
Successful MIAs open the door to other attacks, such as attribute inference and reconstruction attacks.
They can also be used to detect copyright violations or to measure unlearning success.</p>
<p><em>Experiment</em> (Membership Experiment <span class="math notranslate nohighlight">\(Exp^M(\mathcal{A},A,n,\mathcal{D})\)</span>).
Let <span class="math notranslate nohighlight">\(\mathcal{A}\)</span> be an adversary, <span class="math notranslate nohighlight">\(A\)</span> a learning algorithm, <span class="math notranslate nohighlight">\(n\)</span> be a positive integer, and <span class="math notranslate nohighlight">\(\mathcal{D}\)</span> be a distribution over data points <span class="math notranslate nohighlight">\((x,y)\)</span>. The membership experiment proceeds as follows:</p>
<ol class="arabic simple">
<li><p>Sample <span class="math notranslate nohighlight">\(S \sim \mathcal{D}^n\)</span>, and let $A_S = A(S).</p></li>
<li><p>Choose <span class="math notranslate nohighlight">\(b \leftarrow \{0,1\}\)</span> uniformly at random.</p></li>
<li><p>Draw <span class="math notranslate nohighlight">\(z \sim S\)</span> if <span class="math notranslate nohighlight">\(b=0\)</span>, or <span class="math notranslate nohighlight">\(z \sim D\)</span> if <span class="math notranslate nohighlight">\(b=1\)</span>.</p></li>
<li><p><span class="math notranslate nohighlight">\(Exp^M(\mathcal{A},A,n,\mathcal{D})\)</span> is <span class="math notranslate nohighlight">\(1\)</span> if <span class="math notranslate nohighlight">\(\mathcal{A}(z, A_S, n, \mathcal{D}) = b\)</span> and 0 otherwise. <span class="math notranslate nohighlight">\(\mathcal{A}\)</span> must output either <span class="math notranslate nohighlight">\(0\)</span> or <span class="math notranslate nohighlight">\(1\)</span>.</p></li>
</ol>
<p><em>Membership advantage</em>. The membership advantage of <span class="math notranslate nohighlight">\(\mathcal{A}\)</span> is defined as</p>
<div class="math notranslate nohighlight">
\[Adv^M(\mathcal{A}, A, n, \mathcal{D}) = 2 \Pr[Exp^M(\mathcal{A}, A, n, \mathcal{D})=1]-1\]</div>
<p>where probabilities are taken over the coin flips of <span class="math notranslate nohighlight">\(\mathcal{A}\)</span>, the random choices of <span class="math notranslate nohighlight">\(S\)</span> and <span class="math notranslate nohighlight">\(b\)</span>, and the random data point <span class="math notranslate nohighlight">\(z \sim S\)</span> or <span class="math notranslate nohighlight">\(z \sim D\)</span>.</p>
<p>Equivalently, the right-hand side can be expressed as the difference between <span class="math notranslate nohighlight">\(\mathcal{A}\)</span>’s true and false positive rates</p>
<div class="math notranslate nohighlight">
\[ Adv^M = \Pr[\mathcal{A}=0|b=0] - \Pr[\mathcal{A}=0|b=1]\]</div>
<p>where <span class="math notranslate nohighlight">\(Adv^M\)</span> is shortcut for <span class="math notranslate nohighlight">\(Adv^M(\mathcal{A}, A, n, \mathcal{D})\)</span>.</p>
<section id="connection-to-overfitting">
<h3><span class="section-number">8.3.1. </span>Connection to overfitting<a class="headerlink" href="#connection-to-overfitting" title="Link to this heading">#</a></h3>
<p>Overfitting has been shown to predict attacker advantage (Yeom et al. 2018).
In black-box attacks, prediction probabilities (for any classifier) or prediction loss are used to determine membership.
Models, especially when overfit to the training data, display different behavior when encountered with previously seen data.</p>
<p><img alt="alt text" src="_images/mia_overfitting.png" /></p>
<p><video src="_images/evo_non_overfit.mp4" title="alt text"><a href="_images/evo_non_overfit.mp4">alt text</a></video></p>
<p><video src="_images/evo_overfit.mp4" title="alt text"><a href="_images/evo_overfit.mp4">alt text</a></video></p>
<p><img alt="alt text" src="_images/salem.png" /></p>
</section>
</section>
<section id="attribute-inference-attacks">
<h2><span class="section-number">8.4. </span>Attribute inference attacks<a class="headerlink" href="#attribute-inference-attacks" title="Link to this heading">#</a></h2>
<p>In an attribute inference attack, the adversary uses a machine learning model and incomplete information about a data point to infer missing information.
For example, the adversary is given partial information about an individual’s medical record and attempts to infer the individual’s genotype by using a model trained on similar medical records.
Can be obtained from successful MIA.</p>
<p><img alt="alt text" src="_images/aia.png" /></p>
</section>
<section id="reconstruction-attacks">
<h2><span class="section-number">8.5. </span>Reconstruction attacks<a class="headerlink" href="#reconstruction-attacks" title="Link to this heading">#</a></h2>
<p>Reconstruction or model inversion attacks attempt to build the whole training dataset from the information leaked by the trained model.
Can also be obtained from MIAs.
Often use GANs.</p>
<p><img alt="alt text" src="_images/reconstruction.png" /></p>
</section>
<section id="approximate-differential-privacy">
<h2><span class="section-number">8.6. </span>Approximate differential privacy<a class="headerlink" href="#approximate-differential-privacy" title="Link to this heading">#</a></h2>
<p>Let <span class="math notranslate nohighlight">\(\epsilon\)</span> be a positive real number and <span class="math notranslate nohighlight">\(\mathcal{A}\)</span> be a randomized algorithm that takes a dataset as input. Let  <span class="math notranslate nohighlight">\(im \mathcal{A}\)</span> denote the image of <span class="math notranslate nohighlight">\(\mathcal{A}\)</span>. The algorithm <span class="math notranslate nohighlight">\(\mathcal{A}\)</span> is said to provide <span class="math notranslate nohighlight">\((\epsilon,\delta)\)</span>-differential privacy if, for all datasets <span class="math notranslate nohighlight">\(D_1\)</span> and <span class="math notranslate nohighlight">\(D_2\)</span> that differ on a single element (i.e., the data of one person), and all subsets <span class="math notranslate nohighlight">\(S \subset im \mathcal{A}\)</span>:</p>
<div class="math notranslate nohighlight">
\[\Pr[\mathcal{A}(D_1) \in S] \le e^{\epsilon}\Pr[\mathcal{A}(D_2) \in S]+\delta \]</div>
<p>where the probability is taken over the randomness used by the algorithm.</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(\epsilon\)</span> should be close to <span class="math notranslate nohighlight">\(0\)</span> and <span class="math notranslate nohighlight">\(\delta &lt;&lt; 1/|D|\)</span></p></li>
<li><p>Sequential composition: applying mechanisms that are (<span class="math notranslate nohighlight">\(\epsilon_i,\delta_i)\)</span>-differentially private results in <span class="math notranslate nohighlight">\((\sum \epsilon_i, \sum \delta_i)\)</span>-differential privacy.</p></li>
<li><p>Resistance to post-processing: no processing done on data released by a differentially private mechanism will degrade the privacy guarantee.</p></li>
<li><p><span class="math notranslate nohighlight">\((\epsilon,\delta)\)</span>-DP is often achieved via the Gaussian mechanism:</p></li>
</ul>
<div class="math notranslate nohighlight">
\[ \mathcal{M}_{Gauss} = f(x) + \mathcal{M}\left(0, \frac{2\ln(1.25/\delta)(\Delta f)^2}{\epsilon^2} \right) \]</div>
<ul class="simple">
<li><p>Differential privacy bounds the probability of identifying any individual record within a database, parametrized by <span class="math notranslate nohighlight">\(\epsilon\)</span>.</p></li>
<li><p>Results of a DP function are unaltered by the presence or absence of any single record.</p></li>
<li><p>This looks ideal to protect against MIAs in ML.</p></li>
<li><p><span class="math notranslate nohighlight">\(\epsilon\)</span> closer to 0 provide more privacy at the cost of less data utility.</p></li>
<li><p>Relaxations to DP attempt to mitigate this.</p></li>
<li><p>In ML, <span class="math notranslate nohighlight">\((\epsilon,\delta)\)</span>-DP has been applied to stochastic gradient descent, together with the moments accountant.</p></li>
</ul>
<p><img alt="alt text" src="_images/dpsgd.png" /></p>
<p>Yeom provides a bound for the attacker’s advantage in DPML:</p>
<div class="math notranslate nohighlight">
\[Adv \le e^\epsilon - 1\]</div>
<p><span class="math notranslate nohighlight">\(Adv=1\)</span> implies privacy is broken. Therefore, the bound is irrelevant for:</p>
<div class="math notranslate nohighlight">
\[ \epsilon \ge \ln{⁡2} \approx 0.7 \]</div>
<p><img alt="alt text" src="_images/dp_bound.png" /></p>
<ul class="simple">
<li><p>DP-SGD introduces several new hyperparameters to optimize: <span class="math notranslate nohighlight">\(\sigma\)</span>, <span class="math notranslate nohighlight">\(\epsilon\)</span>, <span class="math notranslate nohighlight">\(\delta\)</span>, <span class="math notranslate nohighlight">\(q=L/N\)</span>, <span class="math notranslate nohighlight">\(C\)</span>, <span class="math notranslate nohighlight">\(T\)</span>, which are interdependent.</p></li>
<li><p>To correctly compute the noise to add and not lose additional utility, it should be computed on a per example basis, thus losing most of the benefits of parallelization</p></li>
<li><p>Training times increase of 10x—50x.</p></li>
<li><p>Relaxations of DP make the privacy guarantees even less clear, e.g., the moments accountant.</p></li>
<li><p>High accuracy loss, especially if choosing adequate budgets.</p></li>
</ul>
<p><img alt="alt text" src="_images/dpfy1.png" />
<img alt="alt text" src="_images/dpfy2.png" />
<img alt="alt text" src="_images/dpfy3.png" /></p>
</section>
<section id="machine-unlearning">
<h2><span class="section-number">8.7. </span>Machine unlearning<a class="headerlink" href="#machine-unlearning" title="Link to this heading">#</a></h2>
<p>An unlearning algorithm takes as input a pre-trained model and one or more samples from the train set to unlearn (the “forget set”).
From the model, forget set, and retain set, the unlearning algorithm produces an updated model.
An ideal unlearning algorithm produces a model that is indistinguishable from the model trained without the forget set.
MIA advantage used as a metric.</p>
<p><img alt="alt text" src="_images/unlearning.png" /></p>
</section>
<section id="federated-learning">
<h2><span class="section-number">8.8. </span>Federated learning<a class="headerlink" href="#federated-learning" title="Link to this heading">#</a></h2>
<p>Federated learning (FL) is a machine learning setting where many clients (e.g., mobile devices or whole organizations) collaboratively train a model under the orchestration of a central server, while keeping the training data decentralized.
FL embodies the principles of focused data collection, minimization, separation, and aggregation and can mitigate many of the systemic privacy risks and costs resulting from traditional, centralized machine learning and data science approaches.</p>
<p><img alt="alt text" src="_images/fl.png" /></p>
<p>While FL helps with privacy, it still has some issues to consider:
Privacy issues → White-box access to clients’ contributions.
Security issues → Byzantine, poisoning, adversarial attacks.
Homomorphic encryption, secure multiparty computation, differential privacy at the client level, etc. have been used to tackle some of the privacy issues.
Outlier and anomaly detection mechanisms can be used to detect security attacks.
Some protections against these issues have a negative effect on the other.</p>
<p><img alt="alt text" src="_images/homo-fl.png" /></p>
<p>Secure aggregation protects against a honest-but-curious model manager.
Still, MIAs, AIAs, and reconstruction attacks are still feasible on the aggregated model.
Secure aggregation may prevent the model manager from protecting against security attacks (e.g., byzantine &amp; poisoning attacks).</p>
</section>
<div class="toctree-wrapper compound">
</div>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./."
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
              
              
                <footer class="prev-next-footer d-print-none">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="unstructured.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title"><span class="section-number">7. </span>Privacy in unstructured data</p>
      </div>
    </a>
    <a class="right-next"
       href="notebooks/LipariSC_MIA.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title"><span class="section-number">8.9. </span>Salem paper attacks</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">


  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#machine-learning-pipeline">8.1. Machine learning pipeline</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#threats">8.2. Threats</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#threat-model">8.2.1. Threat model</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#mia">8.3. MIA</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#connection-to-overfitting">8.3.1. Connection to overfitting</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#attribute-inference-attacks">8.4. Attribute inference attacks</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#reconstruction-attacks">8.5. Reconstruction attacks</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#approximate-differential-privacy">8.6. Approximate differential privacy</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#machine-unlearning">8.7. Machine unlearning</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#federated-learning">8.8. Federated learning</a></li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Alberto Blanco-Justicia
</p>

  </div>
  
  <div class="footer-item">
    

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
<div class="extra_footer">
  <p style="text-align: center;">
  <img src="./logos.png" alt="Logos"/>
</p>

</div>
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b"></script>
<script src="_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>